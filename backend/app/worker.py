import os
import json
import uuid
import subprocess
from celery import Celery
from celery.schedules import crontab
from sqlmodel import Session

from app.core.config import settings
from app.core.db import engine
from app.services.workflow_service import workflow_service
from app.services.geo_service import geo_service
from app.services.knowledge_service import knowledge_service
from app.services.sandbox import sandbox_service
from app.models.user import Analysis, CopilotMessage

# 1. åˆå§‹åŒ– Celery åº”ç”¨
celery_app = Celery(
    "worker",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

# 2. é…ç½® Celery
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
)

# å®šæ—¶ä»»åŠ¡ï¼šGEO çŸ¥è¯†åº“åŒæ­¥
celery_app.conf.beat_schedule = {
    "daily-geo-sync": {
        "task": "sync_recent_geo_datasets",
        "schedule": crontab(minute=0, hour=2),
        "args": (15,)
    }
}

# ==========================================
# ä»»åŠ¡ 1ï¼šåŽŸæœ‰æ ‡å‡†æµç¨‹æ‰§è¡Œ
# ==========================================
@celery_app.task(name="run_workflow_task", acks_late=True)
def run_workflow_task(analysis_id: str):
    print(f"ðŸš€ [Celery] Starting task for Analysis ID: {analysis_id}")
    try:
        with Session(engine) as session:
            analysis_uuid = uuid.UUID(analysis_id)
            workflow_service.run_pipeline(session, analysis_uuid)
        return f"Analysis {analysis_id} completed successfully."
    except Exception as e:
        print(f"âŒ [Celery] Task failed: {str(e)}")
        raise e

# ==========================================
# ä»»åŠ¡ 2ï¼šGEO å®šæ—¶åŒæ­¥ä»»åŠ¡
# ==========================================
@celery_app.task(name="sync_recent_geo_datasets")
def sync_recent_geo_datasets(batch_size=15):
    print(f"ðŸ”„ [Cron Task] Starting GEO dataset synchronization (Batch size: {batch_size})...")
    datasets = geo_service.fetch_recent_datasets(retmax=batch_size)
    if not datasets:
        print("âš ï¸ [Cron Task] No datasets fetched. Aborting.")
        return 0
        
    success_count = 0
    with Session(engine) as db:
        for ds in datasets:
            try:
                knowledge_service.ingest_geo_dataset(
                    db=db,
                    accession=ds["accession"],
                    raw_title=ds["title"],
                    raw_summary=ds["summary"],
                    url=ds["url"]
                )
                success_count += 1
            except Exception as e:
                print(f"âŒ [Cron Task] Error ingesting {ds['accession']}: {e}")
                
    print(f"âœ… [Cron Task] GEO sync completed. Successfully processed {success_count}/{len(datasets)} datasets.")
    return success_count

# ==========================================
# ä»»åŠ¡ 3ï¼šAI è°ƒç”¨çš„ Nextflow ä»»åŠ¡ (å¸¦å›žä¼ èŠå¤©è®°å½•åŠŸèƒ½)
# ==========================================
@celery_app.task(name="run_nextflow_pipeline")
def run_nextflow_pipeline(analysis_id: str, project_id: str, workflow_name: str, params: dict, session_id: str = "default"):
    print(f"ðŸš€ Starting background nextflow task {analysis_id}")
    
    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        if not analysis: return
        analysis.status = "running"
        
        work_dir = os.path.join(workflow_service.base_work_dir, analysis_id)
        os.makedirs(work_dir, exist_ok=True)
        analysis.work_dir = work_dir
        db.commit()

    script_dir = os.path.join("/app/pipelines", workflow_name)
    main_script = os.path.join(script_dir, "main.nf")
    
    if not os.path.exists(main_script):
        with Session(engine) as db:
            analysis = db.get(Analysis, uuid.UUID(analysis_id))
            if analysis:
                analysis.status = "failed"
                db.commit()
        return

    cmd = ["nextflow", "run", main_script, "-with-docker", "ubuntu:20.04"]
    for k, v in params.items():
        if isinstance(v, bool):
            if v: cmd.append(f"--{k}")
        else:
            cmd.extend([f"--{k}", str(v)])

    log_file = os.path.join(work_dir, "analysis.log")
    with open(log_file, "w") as f:
        f.write(f"Running command: {' '.join(cmd)}\n")
        f.write("="*50 + "\n")
        
        process = subprocess.Popen(
            cmd, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
        )
        
        # ä¿®å¤ç‚¹ï¼šä¸¥æ ¼ç¼©è¿›ï¼Œè¯»å–è¾“å‡ºå†™å…¥æ—¥å¿—
        for line in process.stdout:
            f.write(line)
            f.flush()
            
        process.wait()

    success = (process.returncode == 0)
    
    # ä»»åŠ¡ç»“æŸï¼Œæ›´æ–°çŠ¶æ€å¹¶å‘å‰ç«¯å›žä¼ èŠå¤©è®°å½•
    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        if analysis:
            analysis.status = "completed" if success else "failed"
        
        status_icon = "âœ…" if success else "âŒ"
        md_msg = f"### {status_icon} Pipeline `{workflow_name}` Finished (ID: `{analysis_id[:8]}`)\n\n"
        if success:
            md_msg += "Execution completed successfully! Please check the **Files** tab to view or download the generated HTML reports and results."
        else:
            md_msg += "Execution failed. Please check the **Workflows** tab and click `âœ¨ AI Diagnose` for details."

        msg = CopilotMessage(project_id=uuid.UUID(project_id), session_id=session_id, role="assistant", content=md_msg)
        db.add(msg)
        db.commit()

# ==========================================
# ä»»åŠ¡ 4ï¼šAI è°ƒç”¨çš„è‡ªå®šä¹‰æ²™ç®±ä»»åŠ¡ (å¸¦å›žä¼ èŠå¤©è®°å½•åŠŸèƒ½)
# ==========================================
@celery_app.task(name="run_sandbox_task")
def run_sandbox_task(analysis_id: str, project_id: str, custom_code: str, session_id: str = "default"):
    print(f"ðŸš€ [Sandbox Task] Starting custom analysis {analysis_id}")
    
    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        if not analysis: return
        analysis.status = "running"
        work_dir = os.path.join(workflow_service.base_work_dir, analysis_id)
        os.makedirs(work_dir, exist_ok=True)
        analysis.work_dir = work_dir
        db.commit()

    log_file = os.path.join(work_dir, "analysis.log")
    with open(log_file, "w", encoding="utf-8") as f:
        f.write("ðŸš€ Starting AI Custom Sandbox Execution...\n")
        f.write("=" * 50 + "\nExecuting Code:\n" + custom_code + "\n" + "=" * 50 + "\n\n")
        
    setup_code = """import os
import warnings
warnings.filterwarnings('ignore')
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd

DATA_DIR = '/data'
WORK_DIR = '/workspace'
os.chdir(WORK_DIR)
"""
    res = sandbox_service.execute_python(project_id, setup_code + "\n" + custom_code)

    with open(log_file, "a", encoding="utf-8") as f:
        if res['stdout']: f.write("STDOUT:\n" + res['stdout'] + "\n")
        if res['stderr']: f.write("STDERR:\n" + res['stderr'] + "\n")
        if res['files']: f.write(f"\nâœ… Generated Files: {[f['name'] for f in res['files']]}\n")
        f.write("\n\nðŸ Execution Finished.\n")

    # ä»»åŠ¡ç»“æŸï¼Œæ›´æ–°çŠ¶æ€å¹¶å‘å‰ç«¯å›žä¼ èŠå¤©è®°å½•åŠç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        if analysis:
            analysis.status = "completed" if res['success'] else "failed"
        
        status_icon = "âœ…" if res['success'] else "âŒ"
        md_msg = f"### {status_icon} Sandbox Analysis Finished (ID: `{analysis_id[:8]}`)\n\n"
        
        if res['files']:
            md_msg += "**Generated Results:**\n"
            for file_info in res['files']:
                fname = file_info if isinstance(file_info, str) else file_info.get('name', str(file_info))
                md_msg += f"- ðŸ“„ `{fname}` (Available in the **Files** tab)\n"
        
        if res['stdout']:
            out = res['stdout'][:1000] + ('...' if len(res['stdout'])>1000 else '')
            md_msg += f"\n**Output Summary:**\n```text\n{out}\n```\n"
            
        if res['stderr']:
            err = res['stderr'][:1000] + ('...' if len(res['stderr'])>1000 else '')
            md_msg += f"\n**Error Detail:**\n```text\n{err}\n```\n"

        msg = CopilotMessage(project_id=uuid.UUID(project_id), session_id=session_id, role="assistant", content=md_msg)
        db.add(msg)
        db.commit()