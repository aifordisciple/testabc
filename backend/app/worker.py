from celery import Celery
from sqlmodel import Session
import uuid

from app.core.config import settings
from app.core.db import engine
from app.models.user import Analysis, CopilotMessage  # ğŸ‘‡ å¯¼å…¥ CopilotMessage
from app.services.workflow_service import workflow_service
from app.services.sandbox import sandbox_service

# 1. åˆå§‹åŒ– Celery åº”ç”¨
celery_app = Celery(
    "worker",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
)

@celery_app.task(name="run_nextflow_pipeline")
def run_nextflow_pipeline(analysis_id: str, project_id: str, workflow_name: str, params: dict, session_id: str = "default"):
    """
    Nextflow ä»»åŠ¡ï¼šæ‰§è¡Œç”Ÿä¿¡åˆ†ææµç¨‹
    ä»»åŠ¡å®Œæˆåï¼Œè‡ªåŠ¨å‘èŠå¤©çª—å£æ¨é€ç»“æœ
    """
    import uuid
    from app.core.db import engine
    from app.models.user import Analysis, CopilotMessage
    from sqlmodel import Session
    from app.services.workflow_service import workflow_service
    import os
    import subprocess
    import time
    
    print(f"ğŸš€ Starting background nextflow task {analysis_id} with session {session_id}")
    
    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        if not analysis: return
        analysis.status = "running"
        work_dir = os.path.join(workflow_service.base_work_dir, analysis_id)
        os.makedirs(work_dir, exist_ok=True)
        analysis.work_dir = work_dir
        db.commit()

    script_dir = os.path.join("/app/pipelines", workflow_name)
    main_script = os.path.join(script_dir, "main.nf")
    
    if not os.path.exists(main_script):
        with Session(engine) as db:
            analysis = db.get(Analysis, uuid.UUID(analysis_id))
            analysis.status = "failed"
            db.commit()
        return

    cmd = ["nextflow", "run", main_script, "-with-docker", "ubuntu:20.04"]
    for k, v in params.items():
        if isinstance(v, bool):
            if v: cmd.append(f"--{k}")
        else:
            cmd.extend([f"--{k}", str(v)])

    log_file = os.path.join(work_dir, "analysis.log")
    with open(log_file, "w") as f:
        f.write(f"Running command: {' '.join(cmd)}\n")
        f.write("="*50 + "\n")
        
    process = subprocess.Popen(
        cmd, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
        )
        
        for line in process.stdout:
            f.write(line)
            f.flush()
            
        process.wait()

    success = (process.returncode == 0)
    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        analysis.status = "completed" if success else "failed"
        db.commit()

    # ğŸ‘‡ æ ¸å¿ƒï¼šä»»åŠ¡ç»“æŸæ—¶ï¼Œä¸»åŠ¨å¾€å¯¹è¯æ¨é€æ¶ˆæ¯ï¼ˆå¸¦ä¸Š session_idï¼‰
    status_icon = "âœ…" if success else "âŒ"
    md_msg = f"### {status_icon} Pipeline `{workflow_name}` Finished (ID: `{analysis_id[:8]}`)\n\n"
    if success:
        md_msg += "Execution completed! ğŸ‰ Please check the **Files** tab to view or download HTML reports and results."
    else:
        md_msg += "Execution failed. Please check the **Workflows** tab and click **âœ¨ AI Diagnose** for details."

    msg = CopilotMessage(
        project_id=uuid.UUID(project_id),
        session_id=session_id,
        role="assistant",
        content=md_msg
    )
    db.add(msg)
    db.commit()


@celery_app.task(name="run_sandbox_task")
def run_sandbox_task(analysis_id: str, project_id: str, custom_code: str, session_id: str = "default"):
    """
    æ²™ç®±ä»»åŠ¡ï¼šæ‰§è¡Œ AI ç”Ÿæˆçš„ Python ä»£ç 
    ä»»åŠ¡å®Œæˆåï¼Œè‡ªåŠ¨å‘èŠå¤©çª—å£æ¨é€ç»“æœ
    """
    import uuid
    from app.core.db import engine
    from app.models.user import Analysis, CopilotMessage
    from sqlmodel import Session
    from app.services.workflow_service import workflow_service
    import os
    
    print(f"ğŸš€ [Sandbox Task] Starting custom analysis {analysis_id} with session {session_id}")
    
    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        if not analysis: return
        analysis.status = "running"
        work_dir = os.path.join(workflow_service.base_work_dir, analysis_id)
        os.makedirs(work_dir, exist_ok=True)
        analysis.work_dir = work_dir
        db.commit()

    log_file = os.path.join(work_dir, "analysis.log")
    with open(log_file, "w", encoding="utf-8") as f:
        f.write("ğŸš€ Starting AI Custom Sandbox Execution...\n")
        f.write("=" * 50 + "\n")
        f.write("Executing Code:\n" + custom_code + "\n")
        f.write("=" * 50 + "\n\n")
        
    setup_code = """import os
import warnings
warnings.filterwarnings('ignore')
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd

DATA_DIR = '/data'
WORK_DIR = '/workspace'
os.chdir(WORK_DIR)
"""
    res = sandbox_service.execute_python(project_id, setup_code + "\n" + custom_code)

    with open(log_file, "a", encoding="utf-8") as f:
        if res['stdout']: f.write("STDOUT:\n" + res['stdout'] + "\n")
        if res['stderr']: f.write("STDERR:\n" + res['stderr'] + "\n")
        if res['files']: f.write(f"\nâœ… Generated Files: {[f['name'] for f in res['files']]}\n")
        f.write("\n\nğŸ Execution Finished.\n")

    with Session(engine) as db:
        analysis = db.get(Analysis, uuid.UUID(analysis_id))
        analysis.status = "completed" if res['success'] else "failed"
        db.commit()

    # ğŸ‘‡ æ ¸å¿ƒï¼šæ²™ç®±ä»»åŠ¡ç»“æŸæ—¶ï¼Œä¸»åŠ¨æ¨é€å¸¦ç»“æœæ–‡ä»¶é¢„è§ˆçš„æ¶ˆæ¯
    status_icon = "âœ…" if res['success'] else "âŒ"
    md_msg = f"### {status_icon} Sandbox Analysis Finished (ID: `{analysis_id[:8]}`)\n\n"
    
    if res['files']:
        md_msg += "**Generated Results:**\n"
        for file_info in res['files']:
            fname = file_info if isinstance(file_info, str) else file_info.get('name', str(file_info))
            md_msg += f"- ğŸ“„ `{fname}` (Available in the **Files** tab)\n"
    
    if res['stdout']:
        out = res['stdout'][:1000] + ('...' if len(res['stdout']) > 1000 else '')
        md_msg += f"\n**Output Summary:**\n```text\n{out}\n```\n"
    
    if res['stderr']:
        err = res['stderr'][:1000] + ('...' if len(res['stderr']) > 1000 else '')
        md_msg += f"\n**Error Detail:**\n```text\n{err}\n```\n"

    msg = CopilotMessage(
        project_id=uuid.UUID(project_id),
        session_id=session_id,
        role="assistant",
        content=md_msg
    )
    db.add(msg)
    db.commit()

# ğŸ‘‡ è¿½åŠ å¯¼å…¥ç›¸å…³çš„åŒ…
from app.services.geo_service import geo_service
from app.services.knowledge_service import knowledge_service
from app.core.db import engine
from sqlmodel import Session

@celery_app.task(name="sync_recent_geo_datasets")
def sync_recent_geo_datasets(batch_size=15):
    """åå°å®šæ—¶ä»»åŠ¡ï¼šæŠ“å–æœ€æ–° GEO æ•°æ®ï¼Œè°ƒç”¨å¤§æ¨¡å‹æ¸…æ´—å¹¶å­˜å…¥å‘é‡åº“"""
    print(f"ğŸ”„ [Cron Task] Starting GEO dataset synchronization (Batch size: {batch_size})...")
    
    datasets = geo_service.fetch_recent_datasets(max=batch_size)
    if not datasets:
        print("âš ï¸ [Cron Task] No datasets fetched. Aborting.")
        return 0
        
    success_count = 0
    with Session(engine) as db:
        for ds in datasets:
            try:
                knowledge_service.ingest_geo_dataset(
                    db=db,
                    accession=ds["accession"],
                    raw_title=ds["title"],
                    raw_summary=ds["summary"],
                    url=ds["url"]
                )
                success_count += 1
            except Exception as e:
                print(f"âŒ [Cron Task] Error ingesting {ds['accession']}: {e}")
                
    print(f"âœ… [Cron Task] GEO sync completed. Successfully processed {success_count}/{len(datasets)} datasets.")
    return success_count
