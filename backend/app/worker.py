from celery import Celery
from sqlmodel import Session
import uuid

from app.core.config import settings
from app.core.db import engine
from app.services.workflow_service import workflow_service

# 1. åˆå§‹åŒ– Celery åº”ç”¨
# æ³¨æ„ï¼šè¿™é‡Œçš„åç§° 'app.worker' å¿…é¡»ä¸ docker-compose command ä¸­çš„ä¸€è‡´
celery_app = Celery(
    "worker",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

# 2. é…ç½® Celery
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
)

# 3. å®šä¹‰å¼‚æ­¥ä»»åŠ¡
@celery_app.task(name="run_workflow_task", acks_late=True)
def run_workflow_task(analysis_id: str):
    """
    Celery ä»»åŠ¡ï¼šæ‰§è¡Œç”Ÿä¿¡åˆ†ææµç¨‹
    æ³¨æ„ï¼šCelery ä»»åŠ¡è¿è¡Œåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­ï¼Œå¿…é¡»åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯
    """
    print(f"ğŸš€ [Celery] Starting task for Analysis ID: {analysis_id}")
    
    try:
        # æ‰‹åŠ¨ç®¡ç† Session ç”Ÿå‘½å‘¨æœŸ
        with Session(engine) as session:
            # å°†å­—ç¬¦ä¸² ID è½¬å› UUID
            analysis_uuid = uuid.UUID(analysis_id)
            
            # è°ƒç”¨æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
            workflow_service.run_pipeline(session, analysis_uuid)
            
        return f"Analysis {analysis_id} completed successfully."
        
    except Exception as e:
        print(f"âŒ [Celery] Task failed: {str(e)}")
        # å®é™…ç”Ÿäº§ä¸­è¿™é‡Œå¯ä»¥è°ƒç”¨ session æ›´æ–° Analysis çŠ¶æ€ä¸º failed
        raise e

# ğŸ‘‡ è¿½åŠ å¯¼å…¥ç›¸å…³çš„åŒ…
from app.services.geo_service import geo_service
from app.services.knowledge_service import knowledge_service
from app.core.db import engine
from sqlmodel import Session

@celery_app.task(name="sync_recent_geo_datasets")
def sync_recent_geo_datasets(batch_size=15):
    """
    åå°å®šæ—¶ä»»åŠ¡ï¼šæŠ“å–æœ€æ–° GEO æ•°æ®ï¼Œè°ƒç”¨å¤§æ¨¡å‹æ¸…æ´—å¹¶å­˜å…¥å‘é‡åº“
    """
    print(f"ğŸ”„ [Cron Task] Starting GEO dataset synchronization (Batch size: {batch_size})...")
    
    # 1. æŠ“å–åŸå§‹æ•°æ®
    datasets = geo_service.fetch_recent_datasets(retmax=batch_size)
    if not datasets:
        print("âš ï¸ [Cron Task] No datasets fetched. Aborting.")
        return 0
        
    success_count = 0
    with Session(engine) as db:
        for ds in datasets:
            try:
                # 2. è°ƒç”¨å·²æœ‰çš„çŸ¥è¯†åº“æœåŠ¡è¿›è¡Œ LLM ç»“æ„åŒ–æå–å’Œå‘é‡åŒ–å…¥åº“
                # æ³¨æ„ï¼šingest_geo_dataset å†…éƒ¨å·²ç»æœ‰æŸ¥é‡æœºåˆ¶ï¼Œé‡åˆ°å·²å­˜åœ¨çš„ä¼šè‡ªåŠ¨è·³è¿‡
                knowledge_service.ingest_geo_dataset(
                    db=db,
                    accession=ds["accession"],
                    raw_title=ds["title"],
                    raw_summary=ds["summary"],
                    url=ds["url"]
                )
                success_count += 1
            except Exception as e:
                print(f"âŒ [Cron Task] Error ingesting {ds['accession']}: {e}")
                
    print(f"âœ… [Cron Task] GEO sync completed. Successfully processed {success_count}/{len(datasets)} datasets.")
    return success_count