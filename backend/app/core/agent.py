import os
from typing import TypedDict, Annotated, Sequence, List, Dict, Any
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END

from app.services.sandbox import sandbox_service

# ==========================================
# 1. å®šä¹‰çŠ¶æ€æœºä¸­çš„ State (å¼•å…¥ operator.add è§£å†³çŠ¶æ€åˆå¹¶)
# ==========================================
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    project_id: str
    extracted_files: Annotated[List[Dict[str, Any]], operator.add]
    iterations: int  # è®°å½•å½“å‰æ€è€ƒå¾ªç¯çš„æ¬¡æ•°

# ==========================================
# 2. åˆå§‹åŒ–å¸¦ Tool Calling çš„ LLM
# ==========================================
def get_llm():
    return ChatOpenAI(
        model=os.getenv("LLM_MODEL", "qwen2.5-coder:32b"),
        base_url=os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434/v1"),
        api_key=os.getenv("LLM_API_KEY", "ollama"),
        temperature=0.1
    )

sandbox_tool = {
    "type": "function",
    "function": {
        "name": "execute_python_sandbox",
        "description": "Execute Python code in a secure sandbox. Used for reading project files and analyzing data.",
        "parameters": {
            "type": "object",
            "properties": {
                "code": {
                    "type": "string",
                    "description": "Python code to execute. Read inputs from '/data'. Save generated plots to '/workspace'. Example: import os; print(os.listdir('/data'))"
                }
            },
            "required": ["code"]
        }
    }
}

# ==========================================
# 3. å®šä¹‰å›¾çš„èŠ‚ç‚¹ (Nodes)
# ==========================================
def agent_node(state: AgentState):
    llm = get_llm()
    iterations = state.get("iterations", 0)
    
    # å¼ºåŒ–ç‰ˆçš„ System Promptï¼ŒåŠ å…¥é˜²å¾¡æ€§æŒ‡ä»¤
    system_prompt = SystemMessage(content=f"""You are Bio-Copilot, an expert bioinformatics AI assistant.
You have access to a secure Python sandbox tool `execute_python_sandbox`.
- User's project data is located in `/data` (Read-Only).
- Output files must be saved to `/workspace`.

CRITICAL RULES:
1. You are currently on iteration {iterations} of maximum 3.
2. If the user asks what files are in the project, write code `import os; print(os.listdir('/data'))` and call the tool.
3. ONCE YOU RECEIVE THE TOOL EXECUTION RESULT, YOU MUST OUTPUT A FINAL CONVERSATIONAL RESPONSE TO THE USER. DO NOT CALL THE TOOL AGAIN.
4. If the tool returns a 'Sandbox system error' or 'docker: not found', APOLOGIZE to the user and STOP. DO NOT RETRY your code.
""")
    
    llm_with_tools = llm.bind_tools([sandbox_tool])
    
    print(f"\nğŸ§  [Agent Node] Invoking LLM (Iteration {iterations})...", flush=True)
    response = llm_with_tools.invoke([system_prompt] + list(state["messages"]))
    
    if response.tool_calls:
        print(f"   => LLM decided to call tool: {response.tool_calls[0]['name']}", flush=True)
    else:
        print(f"   => LLM provided final conversational answer.", flush=True)
        
    # å°†æ–°æ¶ˆæ¯è¿”å›ï¼Œå¹¶å¢åŠ è¿­ä»£æ¬¡æ•°
    return {"messages": [response], "iterations": iterations + 1}

def execute_node(state: AgentState):
    last_message = state["messages"][-1]
    project_id = state["project_id"]
    
    tool_outputs = []
    new_extracted_files = []
    
    for tool_call in last_message.tool_calls:
        if tool_call["name"] == "execute_python_sandbox":
            code = tool_call["args"].get("code", "")
            
            print(f"\n{'='*20} ğŸ› ï¸ SANDBOX EXECUTION {'='*20}", flush=True)
            print(f"Executing Code:\n{code}", flush=True)
            
            # è‡ªåŠ¨æ³¨å…¥ä¸Šä¸‹æ–‡ä»£ç 
            setup_code = "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nDATA_DIR = '/data'\nWORK_DIR = '/workspace'\nos.chdir(WORK_DIR)\n\n"
            final_code = setup_code + code
            
            # è°ƒç”¨æ²™ç®±
            res = sandbox_service.execute_python(project_id, final_code)
            
            # åœ¨åç«¯æ—¥å¿—ä¸­æ‰“å°æ²™ç®±çš„çœŸå®è¾“å‡ºï¼Œä¾¿äºæˆ‘ä»¬ Debug
            print(f"SUCCESS: {res['success']}", flush=True)
            if res['stdout']: print(f"STDOUT:\n{res['stdout'].strip()}", flush=True)
            if res['stderr']: print(f"STDERR:\n{res['stderr'].strip()}", flush=True)
            print(f"{'='*61}\n", flush=True)
            
            # æ ¼å¼åŒ–æ‰§è¡Œç»“æœä¾› LLM é˜…è¯»
            content = f"Execution Success: {res['success']}\n"
            if res['stdout']: content += f"Stdout: {res['stdout'][:2000]}\n"
            if res['stderr']: content += f"Stderr: {res['stderr'][:2000]}\n"
            
            if res['files']:
                content += f"Generated Files: {[f['name'] for f in res['files']]}\n"
                new_extracted_files.extend(res['files'])
                
            tool_outputs.append(ToolMessage(
                content=content,
                tool_call_id=tool_call["id"]
            ))
            
    return {"messages": tool_outputs, "extracted_files": new_extracted_files}

def should_continue(state: AgentState):
    """å†³å®šæ˜¯ç»§ç»­æ‰§è¡Œä»£ç ï¼Œè¿˜æ˜¯ç»“æŸå¯¹è¯è¿”å›ç»™ç”¨æˆ·"""
    last_message = state["messages"][-1]
    iterations = state.get("iterations", 0)
    
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        # å¦‚æœå¤§æ¨¡å‹æ­»å¾ªç¯ï¼Œå¼ºåˆ¶åœ¨ 3 æ¬¡è¿­ä»£ååˆ‡æ–­
        if iterations >= 3:
            print("ğŸ›‘ [Router] Max iterations reached. Forcing Agent to STOP.", flush=True)
            return END
        print("â¡ï¸ [Router] Tool calls detected. Routing to Sandbox Executor.", flush=True)
        return "execute"
        
    print("âœ… [Router] No tool calls detected. Routing to END.", flush=True)
    return END

# ==========================================
# 4. æ„å»ºå¹¶ç¼–è¯‘ LangGraph
# ==========================================
workflow = StateGraph(AgentState)
workflow.add_node("agent", agent_node)
workflow.add_node("execute", execute_node)

workflow.set_entry_point("agent")
workflow.add_conditional_edges("agent", should_continue, {"execute": "execute", END: END})
workflow.add_edge("execute", "agent")

copilot_app = workflow.compile()

# ==========================================
# 5. æš´éœ²ç»™å¤–éƒ¨è°ƒç”¨çš„ä¸»å‡½æ•°
# ==========================================
def run_copilot_agent(project_id: str, history: List[Dict[str, str]]) -> Dict[str, Any]:
    formatted_msgs = []
    for msg in history:
        if msg["role"] == "user":
            formatted_msgs.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            formatted_msgs.append(AIMessage(content=msg["content"]))
            
    initial_state = {
        "messages": formatted_msgs,
        "project_id": project_id,
        "extracted_files": [],
        "iterations": 0
    }
    
    print(f"\nğŸ¬ [Copilot Agent] Starting session for project {project_id}...", flush=True)
    
    # å¢åŠ  LangGraph åŸç”Ÿå®‰å…¨åº•çº¿ï¼Œé˜²æ­¢æ— é™é€’å½’
    final_state = copilot_app.invoke(initial_state, {"recursion_limit": 10})
    
    last_msg = final_state["messages"][-1].content
    files = final_state.get("extracted_files", [])
    
    print(f"ğŸ [Copilot Agent] Session finished. Extracted {len(files)} files.", flush=True)
    
    return {
        "reply": last_msg,
        "files": files
    }