import os
import json
import re
from openai import OpenAI
from typing import Dict, Any, Optional, List

class LLMClient:
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "ollama")
        self.base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434/v1")
        self.api_key = os.getenv("LLM_API_KEY", "ollama")
        # å»ºè®®ä½¿ç”¨ qwen2.5-coder:32bï¼Œå®ƒå¯¹æŒ‡ä»¤éµå¾ªå’Œå¤šè¯­è¨€æ”¯æŒæœ€å¥½
        self.model = os.getenv("LLM_MODEL", "qwen2.5-coder:32b")
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key
        )

    def _clean_response(self, text: str) -> str:
        """æ¸…æ´— DeepSeek/Qwen çš„ <think> æ ‡ç­¾åŠå…¶ä»–æ— å…³å†…å®¹"""
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = re.sub(r'<think>.*', '', text, flags=re.DOTALL)
        return text.strip()

    def _detect_language_request(self, messages: List[Dict[str, str]]) -> str:
        """
        [æ™ºèƒ½è¯­è¨€æ¢æµ‹]
        ä»ç”¨æˆ·å†å²æ¶ˆæ¯ä¸­æ¨æ–­ç›®æ ‡è¯­è¨€ã€‚
        """
        combined_text = " ".join([m.get("content", "").lower() for m in messages])
        
        # 1. æ˜¾å¼ R è¯­è¨€å…³é”®è¯
        if any(kw in combined_text for kw in ["r script", "r language", "write r code", ".r file", "r code"]):
            return "R"
            
        # 2. R è¯­è¨€ç‰¹æœ‰çš„ç”Ÿä¿¡åŒ…/å‡½æ•°å…³é”®è¯ (éšå¼æ¨æ–­)
        r_keywords = [
            "pheatmap", "ggplot", "tidyverse", "deseq2", "seurat", "limma", 
            "bioconductor", "complexheatmap", "shiny", "optparse"
        ]
        if any(kw in combined_text for kw in r_keywords):
            return "R"

        # 3. å…¶ä»–è¯­è¨€
        if "perl" in combined_text:
            return "Perl"
        if "python" in combined_text or "pandas" in combined_text or "matplotlib" in combined_text:
            return "Python"
            
        # é»˜è®¤ Python
        return "Python"

    def _generate_params_schema(self, code: str) -> str:
        """(Fallback) å°è¯•ä»ä»£ç æ–‡æœ¬ä¸­åå‘ç”Ÿæˆç®€å•çš„ JSON Schema"""
        # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å…œåº•ç­–ç•¥ï¼Œé’ˆå¯¹ Pipeline æ¨¡å¼
        return json.dumps({
            "type": "object",
            "properties": {
                "input": {"type": "string", "title": "Input File", "default": None},
                "outdir": {"type": "string", "title": "Output Directory", "default": "./results"}
            }
        }, indent=2)

    def generate_schema_from_code(self, code: str, mode: str) -> str:
        """
        [å‡çº§ç‰ˆ] ä»ä»£ç åå‘è§£æç”Ÿæˆ JSON Schema (Draft-07)
        å¢å¼ºäº†å¯¹ R (optparse) å’Œ Python (argparse) çš„è§£æèƒ½åŠ›ï¼Œæ”¯æŒ Enum å’Œç±»å‹æ¨æ–­ã€‚
        """
        prompt = f"""
You are a Senior Bioinformatics Pipeline Engineer.
Your task is to analyze the provided script code and generate a corresponding **Draft-07 JSON Schema** for its input parameters.
This schema will be used to generate a Web UI form.

### ANALYSIS STRATEGY:

#### 1. Python Scripts (using `argparse`)
- **Scan**: Look for `parser.add_argument(...)`.
- **Name**: Extract `--name` (strip hyphens).
- **Type Mapping**: 
  - `type=str` -> `"string"`
  - `type=int` -> `"integer"`
  - `type=float` -> `"number"`
  - `action='store_true'` -> `"boolean"` (and set `default: false`)
- **Enum**: If `choices=[...]` is present, generate `"enum": [...]`.
- **Default**: Extract `default=...` value.
- **Required**: If `required=True` is set, add parameter name to `"required"` list.

#### 2. R Scripts (using `optparse`)
- **Scan**: Look for `make_option(c("-f", "--flag"), ...)`
- **Name**: Extract long flag `--flag` (strip hyphens).
- **Type Mapping**:
  - `type="character"` -> `"string"`
  - `type="integer"` -> `"integer"`
  - `type="double"` or `type="numeric"` -> `"number"`
  - `type="logical"` -> `"boolean"`
  - `action="store_true"` -> `"boolean"`
- **Default**: Extract `default=...` (handle `NULL` as null).

#### 3. Nextflow
- **Scan**: Look for `params.variable = value` at the top of the file.
- **Type**: Infer from value (quote -> string, number -> number, true/false -> boolean).

### OUTPUT RULES:
1. Return **ONLY** the valid JSON string.
2. The root object must have `"type": "object"` and `"properties"`.
3. Include `"title"`, `"description"` and `"default"` fields where possible.

### SOURCE CODE:
{backticks}
{code}
{backticks}
"""
        messages = [{"role": "user", "content": prompt}]
        
        try:
            print(f"ğŸ” [LLM] Analyzing code to extract schema ({self.model})...")
            
            # å°è¯•å¯ç”¨ JSON æ¨¡å¼ (å¢åŠ ç¡®å®šæ€§)
            kwargs = {}
            # æ³¨æ„: å¹¶éæ‰€æœ‰æ¨¡å‹/ä»£ç†éƒ½æ”¯æŒ response_formatï¼Œè¿™é‡Œåšä¸€ä¸ªå®‰å…¨æ£€æŸ¥
            # å¦‚æœä½ çš„ backend ç¡®å®šæ”¯æŒ OpenAI æ ¼å¼çš„ json_objectï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢ä¸¤è¡Œ
            # kwargs["response_format"] = {"type": "json_object"}

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.1, # ä½æ¸©ä»¥ä¿è¯é€»è¾‘ä¸¥å¯†
                max_tokens=4096,
                **kwargs
            )
            raw_text = response.choices[0].message.content
        except Exception as e:
            print(f"âŒ LLM Error in generate_schema: {e}")
            return json.dumps({"type": "object", "properties": {}}, indent=2)

        # === é²æ£’çš„ JSON æå–é€»è¾‘ ===
        clean_text = self._clean_response(raw_text)
        
        # 1. ä¼˜å…ˆï¼šå°è¯•æå– Markdown ä»£ç å—ä¸­çš„ JSON
        json_block = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', clean_text, re.DOTALL)
        if json_block:
            candidate = json_block.group(1)
        else:
            # 2. å…œåº•ï¼šåˆ©ç”¨å †æ ˆåŒ¹é…å¯»æ‰¾æœ€å¤–å±‚çš„åˆæ³• JSON å¯¹è±¡
            # è¿™èƒ½è§£å†³ "Here is the json: { ... }" è¿™ç§æ··åˆæ–‡æœ¬çš„æƒ…å†µ
            candidate = ""
            stack = 0
            start_idx = -1
            
            for i, char in enumerate(clean_text):
                if char == '{':
                    if stack == 0:
                        start_idx = i
                    stack += 1
                elif char == '}':
                    stack -= 1
                    if stack == 0 and start_idx != -1:
                        # æ‰¾åˆ°é—­åˆçš„ JSON å¯¹è±¡
                        candidate = clean_text[start_idx : i+1]
                        break 
            
            if not candidate:
                candidate = clean_text # å¦‚æœæ²¡æ‰¾åˆ°ç»“æ„ï¼Œå°è¯•è§£ææ•´ä¸ªæ–‡æœ¬

        # 3. éªŒè¯ä¸ä¿®è¡¥
        try:
            schema = json.loads(candidate)
            
            # ç¡®ä¿åŸºæœ¬ç»“æ„å®Œæ•´
            if not isinstance(schema, dict):
                raise ValueError("Parsed JSON is not an object")
            
            if "type" not in schema:
                schema["type"] = "object"
            if "properties" not in schema:
                schema["properties"] = {}
            
            # ä¿®æ­£ä¸€äº›å¸¸è§çš„æ•°æ®ç±»å‹é”™è¯¯
            for prop_name, prop_val in schema.get("properties", {}).items():
                # ç¡®ä¿ enum æ˜¯åˆ—è¡¨
                if "enum" in prop_val and not isinstance(prop_val["enum"], list):
                    del prop_val["enum"]
                
            return json.dumps(schema, indent=2)
            
        except Exception as e:
            print(f"âš ï¸ JSON Parse Failed. Raw snippet: {clean_text[:100]}... Error: {e}")
            # è¿”å›ç©º Schema é˜²æ­¢å‰ç«¯å´©æºƒ
            return json.dumps({"type": "object", "properties": {}}, indent=2)

    def _extract_code_block(self, text: str, mode: str, target_lang: str = "Python") -> Dict[str, Any]:
        """
        [å¼ºå¥çš„æå–å™¨ v3]
        ä½¿ç”¨ target_lang è¿›è¡Œå®šå‘æå–ï¼Œå½»åº•è§£å†³ Script å’Œ JSON æ··æ·†çš„é—®é¢˜ã€‚
        """
        clean_text = self._clean_response(text)
        
        # æå–æ‰€æœ‰ markdown ä»£ç å—: [(lang, content), (lang, content)...]
        blocks = re.findall(r'```(\w*)\n(.*?)```', clean_text, re.DOTALL)
        
        main_code = ""
        params_schema = "{}"
        explanation = re.sub(r'```.*?```', '', clean_text, flags=re.DOTALL).strip()
        explanation = explanation[:300] + "..." if len(explanation) > 300 else explanation

        if not blocks:
            # å…œåº•ï¼šå¦‚æœæ²¡æœ‰ Markdown æ ‡è®°ï¼Œä½†æ–‡æœ¬çœ‹èµ·æ¥åƒä»£ç 
            print("âš ï¸ No code blocks found.")
            if mode != "TOOL" and ("process " in clean_text or "workflow {" in clean_text):
                 return {
                    "main_nf": clean_text,
                    "params_schema": self._generate_params_schema(clean_text),
                    "description": "Raw extraction",
                    "explanation": "No markdown blocks found."
                }
            return self._error_fallback(clean_text, "No markdown code blocks (```) found.")

        # === ç­–ç•¥ï¼šå…ˆæ‰¾ JSON Schemaï¼Œå†æ‰¾ç›®æ ‡è¯­è¨€è„šæœ¬ ===

        # 1. å¯»æ‰¾ Parameters JSON Schema
        for lang, content in blocks:
            l = lang.strip().lower()
            c = content.strip()
            # è¯­è¨€æ ‡è®°æ˜¯ jsonï¼Œæˆ–è€…å†…å®¹çœ‹èµ·æ¥éå¸¸åƒ JSON Schema (åŒ…å« "properties")
            if l == 'json' or (c.startswith('{') and '"properties"' in c):
                try:
                    # éªŒè¯æ˜¯å¦ä¸ºåˆæ³• JSON
                    json.loads(c)
                    params_schema = c
                    break # æ‰¾åˆ° Schemaï¼Œåœæ­¢å¯»æ‰¾
                except:
                    continue

        # 2. å¯»æ‰¾ Main Script (æ ¹æ® target_lang)
        target_tag = target_lang.lower() # r, python, perl...
        
        # 2a. ä¼˜å…ˆåŒ¹é…å‡†ç¡®çš„è¯­è¨€æ ‡ç­¾ (e.g., ```r)
        for lang, content in blocks:
            l = lang.strip().lower()
            if l == target_tag:
                main_code = content
                break
        
        # 2b. å¦‚æœæ²¡æ‰¾åˆ°å‡†ç¡®æ ‡ç­¾ï¼Œå°è¯•æ‰¾é€šç”¨è„šæœ¬æ ‡ç­¾ (é JSON)
        if not main_code:
            supported_langs = ['python', 'r', 'perl', 'bash', 'sh', 'groovy', 'nextflow']
            for lang, content in blocks:
                l = lang.strip().lower()
                c = content.strip()
                # å¦‚æœæ˜¯æ”¯æŒçš„è¯­è¨€ï¼Œä¸”å†…å®¹ä¸ç­‰äºåˆšæ‰æ‰¾åˆ°çš„ schema
                if l in supported_langs and c != params_schema:
                    main_code = content
                    break
        
        # 2c. ç»ˆæå…œåº•ï¼šæ‰¾ç¬¬ä¸€ä¸ªä¸æ˜¯ JSON Schema çš„å—
        if not main_code:
            for lang, content in blocks:
                c = content.strip()
                if c != params_schema and not (c.startswith('{') and '"properties"' in c):
                    main_code = content
                    break

        # 3. åå¤„ç†ï¼šPipeline æ¨¡å¼å¦‚æœæ²¡æœ‰ Schemaï¼Œå°è¯•åå‘ç”Ÿæˆ
        if mode != "TOOL" and params_schema == "{}":
             params_schema = self._generate_params_schema(main_code)

        if not main_code:
            return self._error_fallback(clean_text, f"Could not find a valid {target_lang} script block.")

        return {
            "main_nf": main_code,
            "params_schema": params_schema,
            "description": f"Generated {target_lang} Script",
            "explanation": explanation or "Code generated successfully."
        }

    def _error_fallback(self, raw_text: str, error_msg: str) -> Dict[str, Any]:
        safe_text = raw_text.replace("*/", "* /")
        return {
            "main_nf": f"# GENERATION ERROR\n# {error_msg}\n\n'''\n{safe_text}\n'''",
            "params_schema": "{}",
            "description": "Error",
            "explanation": error_msg
        }

    def _call_llm(self, messages: List[Dict[str, str]]) -> str:
        try:
            print(f"ğŸ¤– Sending request to {self.model}...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.1, 
                max_tokens=8192
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ LLM API Error: {e}")
            return ""

    def _static_analysis(self, code: str, mode: str) -> List[str]:
        """æœ¬åœ°é™æ€è§„åˆ™"""
        errors = []
        if mode in ["PIPELINE", "MODULE"]:
            if re.search(r'output\s*:[^}]*publishDir', code, re.DOTALL):
                errors.append("SYNTAX ERROR: `publishDir` must be placed BEFORE `input:` or `output:` blocks.")
            if re.search(r'def\s+\w+\s*\(.*?\)\s*\{\s*process\s+', code, re.DOTALL):
                errors.append("DSL2 VIOLATION: Do NOT wrap `process` definitions inside Groovy functions.")
        return errors

    def generate_workflow(self, messages: List[Dict[str, str]], mode: str = "MODULE", available_modules: str = "") -> Dict[str, Any]:
        """Agentic Workflow"""
        
        # 1. åŠ¨æ€æ£€æµ‹ç”¨æˆ·æƒ³è¦çš„è¯­è¨€
        target_lang = "Python" # é»˜è®¤
        if mode == "TOOL":
            target_lang = self._detect_language_request(messages)
            print(f"ğŸ¯ Detected Intent Language: {target_lang}")
        elif mode == "MODULE" or mode == "PIPELINE":
            target_lang = "Nextflow"

        print(f"ğŸš€ Step 1: Drafting code for mode: {mode} ({target_lang})...")
        draft_response = self._generate_draft(messages, mode, available_modules, target_lang)
        
        if not draft_response:
             return self._error_fallback("", "LLM Connection Failed")

        # 2. æå–æ—¶ä¼ å…¥ target_langï¼Œç¡®ä¿æå–å‡†ç¡®
        current_data = self._extract_code_block(draft_response, mode, target_lang)
        
        if current_data["main_nf"].startswith("# GENERATION ERROR"):
            return current_data

        # === Step 2: Refine Loop ===
        # åªæœ‰ Nextflow ä»£ç éœ€è¦é™æ€æ£€æŸ¥ï¼Œå·¥å…·è„šæœ¬é€šå¸¸é  LLM è‡ªå·±ä¿è¯
        if mode in ["PIPELINE", "MODULE"]:
            for attempt in range(2):
                code = current_data["main_nf"]
                detected_errors = self._static_analysis(code, mode)
                
                if not detected_errors:
                    print("âœ… Static Analysis passed.")
                    break 
                
                print(f"âš ï¸ Step 2 (Attempt {attempt+1}): Found bugs: {detected_errors}")
                
                refine_prompt = "Your previous code had CRITICAL ERRORS:\n"
                for i, err in enumerate(detected_errors):
                    refine_prompt += f"{i+1}. {err}\n"
                refine_prompt += f"\nPlease rewrite the code in a ```{target_lang.lower()} block."
                
                # Refine ä¹Ÿè¦ä¼ å…¥ target_lang
                current_data = self._refine_code(current_data, refine_prompt, mode, target_lang)

        return current_data

    def _generate_draft(self, messages: List[Dict[str, str]], mode: str, available_modules: str, target_lang: str = "Python") -> str:
        backticks = "`" * 3
        
        base_prompt = """
You are an expert Bioinformatics Developer.
"""
        
        if mode == "TOOL":
            # åŠ¨æ€ç”Ÿæˆ Promptï¼Œé˜²æ­¢ Python å¹²æ‰° R
            lang_instruction = ""
            if target_lang == "R":
                lang_instruction = f"""
- TARGET LANGUAGE: R
- LIBRARY: Use `optparse` for argument parsing.
- STRUCTURE Example:
{backticks}r
library(optparse)
option_list = list(
  make_option(c("-i", "--input"), type="character", default=NULL, help="input file", metavar="character"),
  make_option(c("-o", "--output"), type="character", default="out.pdf", help="output file", metavar="character")
)
opt_parser = OptionParser(option_list=option_list)
opt = parse_args(opt_parser)
# Logic...
{backticks}
"""
            else: # Python
                lang_instruction = f"""
- TARGET LANGUAGE: Python
- LIBRARY: Use `argparse`.
- STRUCTURE Example:
{backticks}python
import argparse
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', required=True)
    args = parser.parse_args()
if __name__ == '__main__':
    main()
{backticks}
"""

            mode_instruction = f"""
MODE: TOOL (Standalone Script)
TASK: Create a production-grade {target_lang} script.

CRITICAL OUTPUT RULES:
1. You MUST output the script inside a ```{target_lang.lower()}``` block.
2. You MUST output the JSON Schema for parameters inside a separate ```json``` block.

{lang_instruction}
"""
        elif mode == "MODULE":
            template = backticks + "groovy\nparams.outdir = './results'\n\nprocess NAME {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/name\", mode: 'copy'\n\n    input:\n    tuple val(meta), path(reads)\n    path index \n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n\n    script:\n    \"\"\"\n    tool_command --threads ${task.cpus} input output\n    \"\"\"\n}\n" + backticks
            mode_instruction = f"MODE: MODULE (Nextflow Process).\nCreate a single `process`. Structure:\n{template}"
        else: 
            code_block = backticks + "groovy\n// Define default parameters\nparams.input = null\nparams.outdir = './results'\n\nChannel.fromPath(params.input)\n    .splitCsv(header:true)\n    .map{ row ->\n        def meta = [id: row.sample_id]\n        def reads = row.r2_path ? [file(row.r1_path), file(row.r2_path)] : [file(row.r1_path)]\n        return tuple(meta, reads)\n    }\n    .set { ch_input }\n" + backticks
            mode_instruction = f"MODE: PIPELINE (Nextflow Workflow).\nStart with standard input logic:\n{code_block}\nAvailable Modules:\n{available_modules}"

        system_prompt = base_prompt + "\n" + mode_instruction
        msgs = [{"role": "system", "content": system_prompt}] + messages
        return self._call_llm(msgs)

    def _refine_code(self, current_data: Dict[str, Any], instructions: str, mode: str, target_lang: str) -> Dict[str, Any]:
        code_to_check = current_data["main_nf"]
        backticks = "`" * 3
        
        refine_prompt = f"""
{instructions}

CURRENT CODE:
{backticks}
{code_to_check}
{backticks}

Return the FIXED code in a ```{target_lang.lower()}``` block.
"""
        msgs = [{"role": "user", "content": refine_prompt}]
        response_text = self._call_llm(msgs)
        return self._extract_code_block(response_text, mode, target_lang)

llm_client = LLMClient()