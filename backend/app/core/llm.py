import os
import json
import re
from openai import OpenAI
from typing import Dict, Any, Optional, List

class LLMClient:
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "ollama")
        self.base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434/v1")
        self.api_key = os.getenv("LLM_API_KEY", "ollama")
        # é»˜è®¤ä½¿ç”¨ deepseek-r1:32b
        self.model = os.getenv("LLM_MODEL", "deepseek-r1:32b")
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key
        )

    def _clean_think_block(self, text: str) -> str:
        """ç§»é™¤ DeepSeek/Qwen çš„ <think> æ€è€ƒè¿‡ç¨‹"""
        # ç§»é™¤æˆå¯¹çš„ think æ ‡ç­¾
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        # ç§»é™¤åªæœ‰å¼€å§‹æ ‡ç­¾çš„æƒ…å†µï¼ˆé˜²æ­¢æˆªæ–­å¯¼è‡´æ®‹ç•™ï¼‰
        text = re.sub(r'<think>.*', '', text, flags=re.DOTALL)
        return text.strip()

    def _extract_json(self, text: str) -> Dict[str, Any]:
        """
        [ä¿®å¤ç‰ˆ] JSON æå–å™¨
        ä¿®å¤äº† NameError: name 'e' is not defined çš„ bug
        """
        # 1. æ¸…æ´—
        cleaned_text = self._clean_think_block(text)
        
        # 2. æš´åŠ›å¯»æ‰¾æœ€å¤–å±‚å¤§æ‹¬å·
        start_idx = cleaned_text.find('{')
        end_idx = cleaned_text.rfind('}')

        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            json_str = cleaned_text[start_idx : end_idx + 1]
        else:
            print(f"âŒ JSON Extract Failed. No curly braces found. Content start: {cleaned_text[:100]}...")
            return self._error_fallback(cleaned_text, "No JSON object found (missing { })")

        # 3. å°è¯•è§£æ
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:  # ğŸ‘ˆ ä¹‹å‰æ¼äº† 'as e'
            print(f"âš ï¸ JSON Decode Error: {e}. Content snippet: {json_str[:100]}...")
            # è¿”å›é”™è¯¯ä¿¡æ¯ç»™å‰ç«¯ï¼Œè€Œä¸æ˜¯è®©åç«¯å´©æºƒ
            return self._error_fallback(json_str, f"Invalid JSON syntax: {str(e)}")

    def _error_fallback(self, raw_text: str, error_msg: str) -> Dict[str, Any]:
        """å…œåº•è¿”å›ï¼Œç¡®ä¿å‰ç«¯æ€»æ˜¯èƒ½æ”¶åˆ°æ•°æ®"""
        # å°†é”™è¯¯ä¿¡æ¯åŒ…è£…æˆæ³¨é‡Šï¼Œå†™å…¥ main_nfï¼Œè¿™æ ·ç”¨æˆ·åœ¨ç¼–è¾‘å™¨é‡Œèƒ½ç›´æ¥çœ‹åˆ°æŠ¥é”™åŸå› 
        safe_raw_text = raw_text.replace("*/", "* /") # é˜²æ­¢æ³¨é‡ŠåµŒå¥—ç ´å
        return {
            "main_nf": f"// AI GENERATION FAILED\n// Error: {error_msg}\n// Please try again.\n\n/* \nRAW AI OUTPUT:\n{safe_raw_text}\n*/", 
            "params_schema": "{}", 
            "description": "Error parsing AI response",
            "explanation": f"AI did not return valid JSON. {error_msg}"
        }

    def _call_llm(self, messages: List[Dict[str, str]]) -> str:
        """ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£"""
        try:
            print(f"ğŸ¤– Sending {len(messages)} messages to {self.model}...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.1, # ä½æ¸©åº¦ï¼Œè¶Šä½è¶Šä¸¥è°¨
                max_tokens=8192
            )
            content = response.choices[0].message.content
            print(f"âœ… LLM Response received ({len(content)} chars).")
            return content
        except Exception as e:
            print(f"âŒ LLM API Error: {e}")
            # è¿™é‡ŒæŠ›å‡ºå¼‚å¸¸ä¼šè¢« FastAPI æ•è·ä¸º 500ï¼Œå¯¼è‡´å‰ç«¯æŠ¥é”™
            # æˆ‘ä»¬æœ€å¥½è¿”å›ä¸€ä¸ªç©ºçš„é”™è¯¯å­—ç¬¦ä¸²ï¼Œè®©ä¸Šå±‚å¤„ç†
            return "{}" 

    def _static_analysis(self, code: str) -> List[str]:
        """æœ¬åœ°é™æ€åˆ†æè§„åˆ™"""
        errors = []

        # è§„åˆ™1: publishDir ä½ç½®
        if re.search(r'output\s*:[^}]*publishDir', code, re.DOTALL):
            errors.append("SYNTAX ERROR: `publishDir` directive MUST be placed BEFORE `input:` or `output:` blocks.")

        # è§„åˆ™2: DSL1 def å®šä¹‰
        if re.search(r'def\s+\w+\s*\(.*?\)\s*\{\s*process\s+', code, re.DOTALL):
            errors.append("DSL2 VIOLATION: Do NOT wrap `process` definitions inside Groovy functions.")

        # è§„åˆ™3: Script å†…ç›´æ¥å¼•ç”¨ params.index
        if re.search(r'--\w+\s+\$\{?params\.\w+(index|ref|genome|db)\w*\}?', code, re.IGNORECASE):
            errors.append("CONTAINER ERROR: Do not use `params.index` in script. Pass it as `input: path index`.")

        return errors

    def generate_workflow(
        self, 
        messages: List[Dict[str, str]], 
        mode: str = "MODULE",
        available_modules: str = ""
    ) -> Dict[str, Any]:
        """
        Agentic Workflow
        """
        
        # === Step 1: Draft ===
        print("ğŸš€ Step 1: Drafting code...")
        draft_response = self._generate_draft(messages, mode, available_modules)
        
        # å¦‚æœè°ƒç”¨ LLM å¤±è´¥ï¼ˆæ¯”å¦‚è¶…æ—¶æˆ–æ²¡è¿ä¸Šï¼‰ï¼Œdraft_response å¯èƒ½æ˜¯ "{}"
        if not draft_response or draft_response == "{}":
             return self._error_fallback("", "LLM API connection failed or returned empty.")

        current_json = self._extract_json(draft_response)
        
        # å¦‚æœ Draft è§£æå¤±è´¥ï¼Œç›´æ¥è¿”å›ï¼Œä¸è¿›è¡Œåç»­ä¿®å¤
        if current_json.get("main_nf", "").startswith("// AI GENERATION FAILED"):
            return current_json

        # === Step 2: Refine Loop ===
        max_attempts = 2
        for attempt in range(max_attempts):
            code = current_json.get("main_nf", "")
            detected_errors = self._static_analysis(code)
            
            if not detected_errors:
                print("âœ… Static Analysis passed.")
                break 
            
            print(f"âš ï¸ Step 2 (Attempt {attempt+1}/{max_attempts}): Found bugs: {detected_errors}")
            
            refine_prompt = "Your previous code had CRITICAL ERRORS. Fix them and return strictly JSON:\n"
            for i, err in enumerate(detected_errors):
                refine_prompt += f"{i+1}. {err}\n"
            
            current_json = self._refine_code(current_json, refine_prompt)
            
            # å¦‚æœä¿®å¤è¿‡ç¨‹ä¸­è§£æå¤±è´¥ï¼Œåœæ­¢å°è¯•
            if current_json.get("main_nf", "").startswith("// AI GENERATION FAILED"):
                break

        # === Step 3: Polish ===
        # åªæœ‰å½“å‰ä»£ç æ­£å¸¸æ—¶æ‰è¿›è¡Œæ¶¦è‰²
        if not current_json.get("main_nf", "").startswith("// AI GENERATION FAILED"):
            print("ğŸ§ Step 3: Final polish...")
            final_checklist = "Ensure `task.cpus` is used. Check .gz handling. Return STRICT JSON."
            current_json = self._refine_code(current_json, final_checklist)
        
        return current_json

    def _generate_draft(self, messages: List[Dict[str, str]], mode: str, available_modules: str) -> str:
        """Step 1 Prompt"""
        backticks = "`" * 3
        
        base_prompt = """
You are an expert Nextflow DSL2 Developer.
Output STRICT JSON only.

ANTI-PATTERNS:
1. NO DSL1 Syntax (def process).
2. NO publishDir inside output.
"""
        
        if mode == "MODULE":
            template = backticks + "groovy" + """
process NAME {
    tag "$meta.id"
    label 'process_medium'
    publishDir "${params.outdir}/name", mode: 'copy'

    input:
    tuple val(meta), path(reads)
    path index 

    output:
    tuple val(meta), path("*.bam"), emit: bam

    script:
    def args = task.ext.args ?: ''
    \"\"\"
    tool_command --threads ${task.cpus} input output
    \"\"\"
}
""" + backticks
            mode_instruction = f"MODE: MODULE. Create a single `process`. Follow this TEMPLATE:\n{template}"
        else: 
            code_block = backticks + "groovy" + """
Channel.fromPath(params.input)
    .splitCsv(header:true)
    .map{ row ->
        def meta = [id: row.sample_id]
        def reads = row.r2_path ? [file(row.r1_path), file(row.r2_path)] : [file(row.r1_path)]
        return tuple(meta, reads)
    }
    .set { ch_input }
""" + backticks
            mode_instruction = f"MODE: PIPELINE. Start with standard input logic:\n{code_block}\nAvailable Modules:\n{available_modules}"

        system_prompt = base_prompt + "\n" + mode_instruction
        msgs = [{"role": "system", "content": system_prompt}] + messages
        return self._call_llm(msgs)

    def _refine_code(self, draft_json: Dict[str, Any], instructions: str) -> Dict[str, Any]:
        """Refine Prompt"""
        code_to_check = draft_json.get("main_nf", "")
        backticks = "`" * 3
        
        refine_prompt = f"""
{instructions}

CURRENT CODE:
{backticks}groovy
{code_to_check}
{backticks}

Output VALID JSON with "main_nf" key.
"""
        msgs = [{"role": "user", "content": refine_prompt}]
        response_text = self._call_llm(msgs)
        return self._extract_json(response_text)

llm_client = LLMClient()