import os
import json
import re
import ast
from openai import OpenAI
from typing import Dict, Any, Optional, List

class LLMClient:
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "ollama")
        self.base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434/v1")
        self.api_key = os.getenv("LLM_API_KEY", "ollama")
        # æ¨èä½¿ç”¨ qwen2.5-coder:32b æˆ– deepseek-r1
        self.model = os.getenv("LLM_MODEL", "qwen2.5-coder:32b")
        
        print(f"ğŸš€ LLM Client Initialized: {self.model} @ {self.base_url}")
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key
        )

    def _clean_response(self, text: str) -> str:
        """æ¸…æ´— DeepSeek/Qwen çš„ <think> æ ‡ç­¾åŠå…¶ä»–æ— å…³å†…å®¹"""
        if not text: return ""
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = re.sub(r'<think>.*', '', text, flags=re.DOTALL) # å¤„ç†æˆªæ–­çš„ think
        return text.strip()

    def _repair_json(self, json_str: str) -> str:
        """
        [JSON ä¿®å¤å¼•æ“ V2]
        å°è¯•ä¿®å¤å¤§æ¨¡å‹è¾“å‡ºçš„éæ ‡å‡† JSON æ ¼å¼ã€‚
        """
        if not json_str: return "{}"
        
        try:
            # 1. ç§»é™¤ Markdown ä»£ç å—æ ‡è®°
            json_str = re.sub(r'^```\w*\s*', '', json_str.strip())
            json_str = re.sub(r'\s*```$', '', json_str.strip())

            # 2. ç§»é™¤æ³¨é‡Š (// å’Œ /* */)
            json_str = re.sub(r'//.*', '', json_str)
            json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)

            # 3. ä¿®å¤å°¾éšé€—å· (Trailing Commas) -> {"a": 1,} -> {"a": 1}
            json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)

            # 4. æ›¿æ¢ Python ç‰¹æœ‰çš„ Boolean/None
            # æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šè¯¯ä¼¤å­—ç¬¦ä¸²ä¸­çš„å†…å®¹ï¼Œä½†åœ¨è§£æ Schema åœºæ™¯ä¸‹é£é™©å¯æ§
            json_str = json_str.replace(": True", ": true").replace(": False", ": false").replace(": None", ": null")
            json_str = json_str.replace(":True", ": true").replace(":False", ": false").replace(":None", ": null")

            return json_str.strip()
        except Exception as e:
            print(f"âš ï¸ [JSON Repair] Exception: {e}")
            return json_str

    def _robust_json_parse(self, text: str) -> Dict[str, Any]:
        """
        [ä¸‰çº§è§£æç­–ç•¥]
        1. ç›´æ¥è§£æ
        2. ä¿®å¤åè§£æ
        3. AST literal_eval (å®¹å¿å•å¼•å·)
        """
        # å°è¯•æå–æœ€å¤–å±‚ {}
        match = re.search(r'(\{.*\})', text, re.DOTALL)
        if match:
            text = match.group(1)

        # ç­–ç•¥ 1: ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 2: ä¿®å¤åè§£æ
        try:
            repaired = self._repair_json(text)
            return json.loads(repaired)
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 3: Python AST è§£æ (å¤„ç†å•å¼•å· key: {'a': 1})
        try:
            # AST è§£æå¯ä»¥å°† Python å­—å…¸å­—ç¬¦ä¸²è½¬ä¸ºå¯¹è±¡ï¼Œç„¶åæˆ‘ä»¬å† dump æˆæ ‡å‡† JSON
            obj = ast.literal_eval(text)
            if isinstance(obj, dict):
                return obj
        except:
            pass

        print(f"âŒ [JSON Parse] All strategies failed for: {text[:100]}...")
        return {}

    def _call_llm(self, messages: List[Dict[str, str]], json_mode: bool = False) -> str:
        """
        ç»Ÿä¸€è°ƒç”¨æ¥å£ï¼Œé›†æˆæ—¥å¿—å’Œ JSON Mode
        """
        try:
            print(f"\n{'='*20} ğŸ“¤ LLM REQUEST ({self.model}) {'='*20}")
            print(json.dumps(messages, indent=2, ensure_ascii=False))
            print(f"{'='*60}")

            kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": 0.1, # ä¿æŒä½æ¸©ä»¥è·å¾—ç¨³å®šè¾“å‡º
                "max_tokens": 8192
            }

            # å°è¯•å¼€å¯ JSON Mode (å…¼å®¹ OpenAI API æ ¼å¼)
            if json_mode:
                kwargs["response_format"] = {"type": "json_object"}
                print("ğŸ’¡ JSON Mode: ENABLED")

            response = self.client.chat.completions.create(**kwargs)
            content = response.choices[0].message.content

            print(f"\n{'='*20} ğŸ“¥ LLM RESPONSE {'='*20}")
            print(content)
            print(f"{'='*60}\n")

            return content

        except Exception as e:
            print(f"âŒ LLM API Error: {e}")
            return ""

    def generate_schema_from_code(self, code: str, mode: str) -> str:
        """
        [ä¸“é—¨ç”¨é€”] ä»ä»£ç åå‘ç”Ÿæˆ JSON Schemaã€‚
        ä½¿ç”¨äº† Few-Shot Prompting ä»¥æé«˜å‡†ç¡®ç‡ã€‚
        """
        # æ„å»º Few-Shot Prompt
        prompt = f"""
You are a Parser Agent. Analyze the code and extract input parameters into a **Draft-07 JSON Schema**.

### EXAMPLES

**Input (Python):**
parser.add_argument('--input', type=str, required=True, help="Input file")
parser.add_argument('--threads', type=int, default=4)

**Output (JSON):**
{{
  "type": "object",
  "properties": {{
    "input": {{ "type": "string", "description": "Input file" }},
    "threads": {{ "type": "integer", "default": 4 }}
  }},
  "required": ["input"]
}}

**Input (R):**
make_option(c("-f", "--file"), type="character", default=NULL)
make_option(c("--verbose"), action="store_true", default=FALSE)

**Output (JSON):**
{{
  "type": "object",
  "properties": {{
    "file": {{ "type": "string", "default": null }},
    "verbose": {{ "type": "boolean", "default": false }}
  }}
}}

### YOUR TASK
Extract parameters from the following code.
Return ONLY valid JSON.

Code:
```
{code}
```
"""
        messages = [{"role": "user", "content": prompt}]
        
        # å¼ºåˆ¶å¯ç”¨ JSON Mode
        response = self._call_llm(messages, json_mode=True)
        cleaned = self._clean_response(response)
        
        parsed_obj = self._robust_json_parse(cleaned)
        
        # å…œåº•ï¼šå¦‚æœè§£æå¤±è´¥æˆ–ä¸ºç©ºï¼Œè¿”å›é»˜è®¤
        if not parsed_obj or "properties" not in parsed_obj:
            print("âš ï¸ Schema extraction failed, returning default.")
            return json.dumps({
                "type": "object", 
                "properties": {
                    "input": {"type": "string", "title": "Input File (Auto-detected)"}
                }
            }, indent=2)
            
        return json.dumps(parsed_obj, indent=2)

    def _extract_code_block(self, text: str, mode: str, target_lang: str = "Python") -> Dict[str, Any]:
        """
        [æå–ä¸åå¤„ç†]
        ä» LLM å›å¤ä¸­æå–ä»£ç å—å’Œ JSON Schemaã€‚å¦‚æœ Schema ç¼ºå¤±ï¼Œè‡ªåŠ¨è°ƒç”¨ç”Ÿæˆå™¨è¡¥å……ã€‚
        """
        clean_text = self._clean_response(text)
        
        # 1. æå–æ‰€æœ‰ Markdown ä»£ç å—
        # åŒ¹é… ```lang ... ```
        blocks = re.findall(r'```(\w*)\n(.*?)```', clean_text, re.DOTALL)
        
        main_code = ""
        params_schema_str = ""
        
        # 2. éå†å—è¿›è¡Œåˆ†ç±»
        target_lang_lower = target_lang.lower()
        
        for lang, content in blocks:
            lang = lang.strip().lower()
            content = content.strip()
            
            # åˆ¤æ–­æ˜¯å¦ä¸º JSON Schema
            if lang == 'json' or (content.startswith('{') and '"properties"' in content):
                # ç®€å•éªŒè¯æ˜¯å¦çœ‹èµ·æ¥åƒ Schema
                if '"type":' in content or '"properties":' in content:
                    params_schema_str = content
                    continue
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡ä»£ç 
            # ä¼˜å…ˆåŒ¹é…å‡†ç¡®çš„è¯­è¨€æ ‡ç­¾
            if lang == target_lang_lower:
                main_code = content
            # å…¶æ¬¡åŒ¹é…å¸¸è§çš„è„šæœ¬è¯­è¨€ (å¦‚æœè¿˜æ²¡æ‰¾åˆ°)
            elif not main_code and lang in ['python', 'r', 'perl', 'bash', 'sh', 'groovy', 'nextflow']:
                main_code = content
            # æœ€åï¼Œå¦‚æœæ²¡æœ‰è¯­è¨€æ ‡ç­¾ï¼Œä½†å†…å®¹ä¸åƒ JSONï¼Œä¹Ÿå¯èƒ½æ˜¯ä»£ç 
            elif not main_code and not lang and not content.startswith('{'):
                main_code = content

        # 3. å…œåº•ç­–ç•¥ï¼šå¦‚æœæ²¡æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•ç›´æ¥ä»æ–‡æœ¬ä¸­æå–
        if not main_code and not blocks:
            # å‡è®¾æ•´ä¸ªæ–‡æœ¬å°±æ˜¯ä»£ç  (å¦‚æœæ˜¯ Pipeline æ¨¡å¼)
            if mode != "TOOL":
                main_code = clean_text
            else:
                return self._error_fallback(clean_text, f"No ```{target_lang}``` code block found.")

        # 4. Schema è‡ªåŠ¨è¡¥å…¨/ä¿®å¤
        # å¦‚æœæ²¡æœ‰æå–åˆ° Schemaï¼Œæˆ–è€…æå–åˆ°çš„æ— æ³•è§£æï¼Œåˆ™è°ƒç”¨ä¸“é—¨çš„ç”Ÿæˆå™¨
        valid_schema = False
        if params_schema_str:
            parsed = self._robust_json_parse(params_schema_str)
            if parsed and "properties" in parsed:
                params_schema_str = json.dumps(parsed, indent=2)
                valid_schema = True
        
        if not valid_schema and main_code:
            print(f"ğŸ”„ Missing or invalid schema. Invoking specialized schema generator...")
            params_schema_str = self.generate_schema_from_code(main_code, mode)

        return {
            "main_nf": main_code,
            "params_schema": params_schema_str,
            "description": f"Generated {target_lang} Script",
            "explanation": clean_text[:200] + "..." # ç®€ç•¥è¯´æ˜
        }

    def _detect_language_request(self, messages: List[Dict[str, str]]) -> str:
        content = " ".join([m.get("content", "").lower() for m in messages])
        if "r script" in content or "rè„šæœ¬" in content or "r language" in content or "library(" in content:
            return "R"
        if "perl" in content:
            return "Perl"
        return "Python" # Default

    def _error_fallback(self, raw_text: str, error_msg: str) -> Dict[str, Any]:
        return {
            "main_nf": f"# GENERATION ERROR: {error_msg}\n\n'''\n{raw_text}\n'''",
            "params_schema": "{}",
            "description": "Error",
            "explanation": error_msg
        }

    def _static_analysis(self, code: str, mode: str) -> List[str]:
        """ç®€å•çš„é™æ€æ£€æŸ¥ï¼Œå‘ç°ä¸¥é‡è¯­æ³•é”™è¯¯"""
        errors = []
        if mode in ["PIPELINE", "MODULE"]:
            if "process " in code and "workflow " not in code and mode == "PIPELINE":
                # è¿™æ˜¯ä¸€ä¸ª Warningï¼Œä¸ç®— Error
                pass
            if re.search(r'output\s*:[^}]*publishDir', code, re.DOTALL):
                errors.append("Nextflow Syntax Error: `publishDir` must be defined BEFORE `input:` or `output:` blocks.")
        return errors

    def _generate_draft(self, messages: List[Dict[str, str]], mode: str, available_modules: str, target_lang: str) -> str:
        """æ ¹æ®æ¨¡å¼æ„å»º System Prompt å¹¶è¯·æ±‚åˆç¨¿"""
        
        # åŸºç¡€è®¾å®š
        system_prompt = "You are an expert Bioinformatics Developer. Write clean, production-ready code."
        
        if mode == "TOOL":
            if target_lang == "R":
                template = """
library(optparse)
option_list = list(
    make_option(c("-i", "--input"), type="character", help="Input file"),
    make_option(c("-o", "--output"), type="character", default="result.txt", help="Output file")
)
opt = parse_args(OptionParser(option_list=option_list))
# ... logic ...
"""
                instruct = f"Write an R script using `optparse`. Follow this structure:\n```{template}```"
            else:
                template = """
import argparse
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', required=True, help="Input file")
    parser.add_argument('--output', default="result.txt", help="Output file")
    args = parser.parse_args()
    # ... logic ...
if __name__ == "__main__":
    main()
"""
                instruct = f"Write a Python script using `argparse`. Follow this structure:\n```{template}```"
                
            system_prompt += f"\n\nMODE: TOOL ({target_lang})\n{instruct}\n\nIMPORTANT: Output the script in a ```{target_lang.lower()}``` block."

        elif mode == "MODULE":
            system_prompt += "\n\nMODE: Nextflow Module.\nWrite a single `process` block. Use `publishDir` to save outputs."
            
        elif mode == "PIPELINE":
            system_prompt += f"\n\nMODE: Nextflow Pipeline.\nConnect processes in a `workflow` block.\nAvailable Modules Context:\n{available_modules}"

        # ç»„åˆæ¶ˆæ¯
        full_messages = [{"role": "system", "content": system_prompt}] + messages
        return self._call_llm(full_messages)

    def _refine_code(self, current_data: Dict[str, Any], feedback: str, mode: str, target_lang: str) -> Dict[str, Any]:
        """è‡ªæˆ‘ä¿®æ­£å¾ªç¯"""
        code = current_data["main_nf"]
        prompt = f"""
The previous code had issues. Please FIX it based on the feedback.

FEEDBACK:
{feedback}

ORIGINAL CODE:
{code}


Return the full FIXED code in a ```{target_lang.lower()}``` block.
"""
        messages = [{"role": "user", "content": prompt}]
        response = self._call_llm(messages)
        return self._extract_code_block(response, mode, target_lang)

    def generate_workflow(self, messages: List[Dict[str, str]], mode: str = "MODULE", available_modules: str = "") -> Dict[str, Any]:
        """
        ä¸»å…¥å£ï¼šç”Ÿæˆå·¥ä½œæµæˆ–è„šæœ¬
        """
        # 1. ç¡®å®šç›®æ ‡è¯­è¨€
        target_lang = "Python"
        if mode == "TOOL":
            target_lang = self._detect_language_request(messages)
        elif mode in ["MODULE", "PIPELINE"]:
            target_lang = "Nextflow"

        print(f"ğŸ¬ Starting Generation Task: {mode} ({target_lang})")

        # 2. ç”Ÿæˆåˆç¨¿
        draft_text = self._generate_draft(messages, mode, available_modules, target_lang)
        if not draft_text:
            return self._error_fallback("", "LLM Empty Response")

        # 3. æå–ä»£ç å’Œå‚æ•°
        data = self._extract_code_block(draft_text, mode, target_lang)
        
        # 4. (å¯é€‰) é™æ€æ£€æŸ¥ä¸è‡ªæˆ‘ä¿®æ­£å¾ªç¯
        # å¯¹äº Pipeline/Moduleï¼Œè¿›è¡Œç®€å•çš„è¯­æ³•æ£€æŸ¥
        if mode in ["PIPELINE", "MODULE"]:
            errors = self._static_analysis(data["main_nf"], mode)
            if errors:
                print(f"âš ï¸ Static Analysis failed: {errors}. Attempting auto-fix...")
                data = self._refine_code(data, "\n".join(errors), mode, target_lang)

        return data

llm_client = LLMClient()