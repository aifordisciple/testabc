from fastapi import APIRouter, Depends, HTTPException
from sqlmodel import Session, select
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uuid

from app.core.db import get_session
from app.api.deps import get_current_user
from app.models.user import User, Project
from app.models.bio import WorkflowTemplate
from app.core.llm import llm_client

# å¼•å…¥åˆšåˆšå†™çš„æ²™ç®±æœåŠ¡
from app.services.sandbox import sandbox_service

router = APIRouter()

# ================================
# 1. ç»“æ„ä¸æ¨¡å‹å®šä¹‰ (ä¿ç•™ä½ çš„åŸæœ‰ç»“æ„)
# ================================
class ChatMessage(BaseModel):
    role: str 
    content: str

class GenerateRequest(BaseModel):
    messages: List[ChatMessage]
    mode: str = "MODULE" 
    current_code: Optional[str] = None

class GenerateResponse(BaseModel):
    main_nf: str
    params_schema: str
    description: str
    explanation: str

class ParseParamsRequest(BaseModel):
    code: str
    mode: str = "TOOL" # PIPELINE / TOOL


# ================================
# 2. ä»£ç ç”Ÿæˆç«¯ç‚¹ (æ¢å¤ä½ åŸæœ‰çš„ä¸°å¯Œé€»è¾‘)
# ================================
@router.post("/generate", response_model=GenerateResponse)
async def generate_workflow_code(
    payload: GenerateRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆ Nextflow/Tool ä»£ç ï¼Œæ”¯æŒä¸Šä¸‹æ–‡ã€‚
    """
    conversation = [m.model_dump() for m in payload.messages]
    
    # æ¢å¤ï¼šä¸Šä¸‹æ–‡ä»£ç æ³¨å…¥
    if payload.current_code and len(conversation) > 0:
        last_msg = conversation[-1]
        if last_msg['role'] == 'user':
            last_msg['content'] += f"\n\n[Current Code Context]:\n{payload.current_code}"

    # æ¢å¤ï¼šPipeline æ¨¡å¼ä¸‹è·å–ç°æœ‰æ¨¡å—
    available_modules_str = ""
    if payload.mode == "PIPELINE":
        modules = session.exec(select(WorkflowTemplate).where(WorkflowTemplate.workflow_type == "MODULE")).all()
        if modules:
            module_list = []
            for m in modules:
                module_list.append(f"- Module Name: {m.name}\n  Description: {m.description}")
            available_modules_str = "\n".join(module_list)
        else:
            available_modules_str = "No existing modules found in database."

    try:
        result = llm_client.generate_workflow(
            messages=conversation,
            mode=payload.mode,
            available_modules=available_modules_str
        )
        return GenerateResponse(**result)
    except Exception as e:
        print(f"AI Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ================================
# 3. ä»£ç åå‘è§£ææ¥å£ (æ¢å¤)
# ================================
@router.post("/parse_params")
async def parse_params(
    payload: ParseParamsRequest,
    current_user: User = Depends(get_current_user)
):
    """
    åˆ†ææäº¤çš„ä»£ç ï¼Œåå‘æå–å‚æ•°å¹¶ç”Ÿæˆ JSON Schema
    """
    if not payload.code.strip():
        raise HTTPException(status_code=400, detail="Code is empty")
        
    try:
        # è°ƒç”¨ llm.py ä¸­çš„æ–°æ–¹æ³• (ç°åœ¨ç”± instructor é©±åŠ¨)
        schema_str = llm_client.generate_schema_from_code(payload.code, payload.mode)
        return {"params_schema": schema_str}
    except Exception as e:
        print(f"Error in parse_params: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ================================
# 4. [æ–°å¢] å®‰å…¨æ²™ç®±æ‰§è¡Œç«¯ç‚¹
# ================================
class ExecuteRequest(BaseModel):
    code: str

@router.post("/projects/{project_id}/sandbox/execute")
def execute_sandbox_code(
    project_id: uuid.UUID,
    payload: ExecuteRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    åœ¨å®‰å…¨çš„ Docker æ²™ç®±ä¸­æ‰§è¡Œ Python ä»£ç ã€‚
    ä¸“é—¨ä¾› Bio-Copilot ä»£ç†åœ¨åˆ†ææ•°æ®æ—¶è°ƒç”¨ã€‚
    """
    # éªŒè¯é¡¹ç›®æƒé™
    project = session.get(Project, project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found or permission denied.")
        
    if not payload.code.strip():
        raise HTTPException(status_code=400, detail="Code cannot be empty.")

    # æ³¨å…¥éšè—çš„å¼•å¯¼ä»£ç ï¼Œå¸®åŠ©æ¨¡å‹æ›´å®¹æ˜“åœ°æ‰¾åˆ°é¡¹ç›®æ•°æ®
    setup_code = """import os
import sys
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# Set working directory contexts
DATA_DIR = '/data'      # Project files (Read-Only)
WORK_DIR = '/workspace' # Output files (Read-Write)
os.chdir(WORK_DIR)

"""
    # æ‹¼æ¥å¼•å¯¼ä»£ç å’Œ AI ç”Ÿæˆçš„å®é™…ä»£ç 
    final_code = setup_code + payload.code

    try:
        # æ‰§è¡Œä»£ç 
        result = sandbox_service.execute_python(
            project_id=str(project_id),
            code=final_code,
            timeout=60 # è®¾ç½® 60 ç§’è¶…æ—¶
        )
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Sandbox Execution Error: {str(e)}")

# ğŸ‘‡ è¿½åŠ å¯¼å…¥ (æ”¾åœ¨æ–‡ä»¶é¡¶éƒ¨ä¹Ÿå¯ä»¥ï¼Œä½†ä¸è¦†ç›–åŸæœ‰ä»£ç )
from app.core.agent import run_copilot_agent

class CopilotChatRequest(BaseModel):
    messages: List[Dict[str, str]]

@router.post("/projects/{project_id}/copilot/chat")
def chat_with_copilot(
    project_id: uuid.UUID,
    payload: CopilotChatRequest,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    Bio-Copilot ä¼šè¯æ¥å£ã€‚
    è§¦å‘ LangGraph ä»£ç†ï¼Œè‡ªä¸»åˆ†æéœ€æ±‚ã€ç”Ÿæˆä»£ç å¹¶åœ¨æ²™ç®±ä¸­æ‰§è¡Œå‡ºå›¾ã€‚
    """
    project = session.get(Project, project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")

    try:
        # è°ƒç”¨ LangGraph Agent å¤§è„‘
        result = run_copilot_agent(str(project_id), payload.messages)
        return result
    except Exception as e:
        print(f"Copilot Error: {str(e)}", flush=True)
        raise HTTPException(status_code=500, detail=str(e))

# ğŸ‘‡ è¿½åŠ å¯¼å…¥
from app.models.user import Analysis
from app.services.workflow_service import workflow_service
from langchain_core.messages import SystemMessage, HumanMessage
import os

class DiagnoseResponse(BaseModel):
    diagnosis: str

@router.post("/projects/{project_id}/analyses/{analysis_id}/diagnose", response_model=DiagnoseResponse)
def diagnose_analysis_error(
    project_id: uuid.UUID,
    analysis_id: uuid.UUID,
    session: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    æ™ºèƒ½é”™è¯¯è¯Šæ–­æ¥å£ï¼šè¯»å–å¤±è´¥ä»»åŠ¡çš„æœ€å 150 è¡Œæ—¥å¿—ï¼Œè°ƒç”¨ LLM åˆ†ææŠ¥é”™åŸå› ã€‚
    """
    # 1. æƒé™ä¸è®°å½•éªŒè¯
    project = session.get(Project, project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=404, detail="Project not found")
        
    analysis = session.get(Analysis, analysis_id)
    if not analysis or analysis.project_id != project_id:
        raise HTTPException(status_code=404, detail="Analysis not found")
        
    # 2. è¯»å–æ—¥å¿—æ–‡ä»¶ (å–æœ€å 150 è¡Œ)
    base_dir = analysis.work_dir if analysis.work_dir else os.path.join(workflow_service.base_work_dir, str(analysis.id))
    log_path = os.path.join(base_dir, "analysis.log")
    
    if not os.path.exists(log_path):
        raise HTTPException(status_code=404, detail="Log file not found.")
        
    try:
        with open(log_path, "r", encoding="utf-8", errors="replace") as f:
            lines = f.readlines()
            tail_lines = lines[-150:]
            error_log = "".join(tail_lines)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading log: {str(e)}")

    if not error_log.strip():
        return DiagnoseResponse(diagnosis="Log file is empty. The task might not have started properly.")

    # 3. æ„å»º Prompt å¹¶è°ƒç”¨åŸç”Ÿ Langchain LLM
    from app.core.agent import get_llm 
    llm = get_llm()
    
    system_prompt = SystemMessage(content="""You are a Senior Bioinformatics DevOps Engineer. 
Your task is to analyze failed execution logs (Nextflow, Docker, Python, or R) and provide a concise, accurate diagnosis.
Output format:
1. **Root Cause**: (What went wrong in simple terms)
2. **Detailed Analysis**: (Explain the specific log error)
3. **Actionable Fix**: (What the user should do to fix it. e.g., 'Increase memory to 4GB', 'Check if input FASTQ is empty', 'Fix parameter typo')
Use Markdown. Be extremely precise and helpful.
""")

    # ä¿®å¤ç‚¹ï¼šç§»é™¤äº†è¿™é‡Œçš„ Markdown ä¸‰å¼•å·ï¼Œä½¿ç”¨ç ´æŠ˜å·æ›¿ä»£ï¼Œé˜²æ­¢ä»£ç å—è¢«æ„å¤–æˆªæ–­
    user_prompt = HumanMessage(content=f"""Here is the tail of the failed log for workflow '{analysis.workflow}':

---
{error_log}
---

Please diagnose the error.""")

    try:
        print(f"ğŸ©º [Auto-Debug] Diagnosing analysis {analysis_id}...", flush=True)
        response = llm.invoke([system_prompt, user_prompt])
        return DiagnoseResponse(diagnosis=response.content)
    except Exception as e:
        print(f"Diagnosis LLM Error: {str(e)}", flush=True)
        raise HTTPException(status_code=500, detail="AI diagnosis failed to generate.")