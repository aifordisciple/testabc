import os
import json
import uuid
import instructor
from openai import OpenAI
from pydantic import BaseModel, Field
from typing import Optional, List
from sqlmodel import Session, select
from app.models.knowledge import PublicDataset
from app.models.user import Project, File, ProjectFileLink

# ==========================================
# Pydantic ç»“æ„åŒ–æ¨¡å‹å®šä¹‰
# ==========================================

class StructuredMetadata(BaseModel):
    """ç”¨äºå­˜å‚¨å¤§æ¨¡å‹æ¸…æ´—åçš„é«˜åº¦ç»“æ„åŒ–å…ƒæ•°æ®"""
    organism: Optional[str] = Field(None, description="The biological species, e.g., 'Homo sapiens', 'Mus musculus'")
    disease_state: Optional[str] = Field(None, description="The disease or condition studied, e.g., 'Lung Cancer', 'Healthy', 'Normal'")
    sample_count: int = Field(0, description="Total number of samples in the dataset")
    cleaned_summary: str = Field(..., description="A concise, easily readable summary of the dataset's purpose and experimental design")

class LLMGeneratedDataset(BaseModel):
    """ç”¨äºçº¦æŸå¤§æ¨¡å‹ç›´æ¥æœç´¢å¹¶è¿”å›çš„æ•°æ®é›†æ ¼å¼"""
    accession: str = Field(..., description="GEO Accession number, e.g., GSE12345. Must be accurate.")
    title: str = Field(..., description="Title of the dataset")
    summary: str = Field(..., description="Detailed summary of the experimental design and purpose")
    url: str = Field(..., description="URL to the dataset, typically https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=...")

class LLMDatasetSearchResult(BaseModel):
    """å¤§æ¨¡å‹æœç´¢ç»“æœåˆ—è¡¨"""
    datasets: List[LLMGeneratedDataset] = Field(description="List of highly relevant public datasets matching the user query")

# ==========================================
# Knowledge Service æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
# ==========================================

class KnowledgeService:
    def __init__(self):
        # å‚æ•°ç³»ç»Ÿè®¾ç½®
        # self.base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434/v1")
        # self.api_key = os.getenv("LLM_API_KEY", "ollama")
        # self.llm_model = os.getenv("LLM_MODEL", "qwen2.5-coder:32b")
        # self.embed_model = os.getenv("EMBEDDING_MODEL", "bge-m3")

        self.base_url = "http://host.docker.internal:11434/v1"
        self.api_key = "ollama"
        self.llm_model = "qwen3.5:cloud"
        # é»˜è®¤ä½¿ç”¨ bge-m3 ä½œä¸ºå‘é‡æ¨¡å‹
        self.embed_model = "bge-m3"

        # åˆå§‹åŒ–å¸¦ JSON çº¦æŸçš„ LLM å®¢æˆ·ç«¯
        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key)
        self.instructor_client = instructor.from_openai(self.client, mode=instructor.Mode.JSON)

    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        response = self.client.embeddings.create(
            input=text.replace("\n", " "),
            model=self.embed_model
        )
        return response.data[0].embedding

    def clean_metadata_with_llm(self, raw_text: str) -> StructuredMetadata:
        """è°ƒç”¨å¤§æ¨¡å‹æ·±åº¦æ¸…æ´—åŸå§‹æ–‡æœ¬ï¼Œæå–å®éªŒå…³é”®ä¿¡æ¯"""
        prompt = f"Extract the key experimental metadata from the following dataset description:\n\n{raw_text}"
        metadata = self.instructor_client.chat.completions.create(
            model=self.llm_model,
            response_model=StructuredMetadata,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_retries=2
        )
        return metadata

    def ingest_geo_dataset(self, db: Session, accession: str, raw_title: str, raw_summary: str, url: str) -> PublicDataset:
        """
        æ ¸å¿ƒ ETL å…¥åº“é€»è¾‘ï¼šåŒ…å«ä¸¥æ ¼çš„æŸ¥é‡é˜²å¾¡
        """
        # ğŸ›¡ï¸ æŸ¥é‡æœºåˆ¶ï¼šå¦‚æœæ•°æ®åº“å·²ç»å­˜åœ¨è¯¥ GEO å·ï¼Œç›´æ¥è·³è¿‡å¹¶è¿”å›æœ¬åœ°è®°å½•
        existing = db.exec(select(PublicDataset).where(PublicDataset.accession == accession)).first()
        if existing:
            print(f"âœ… [Knowledge ETL] Dataset {accession} already in DB. Skipping processing.", flush=True)
            return existing
            
        print(f"ğŸ§  [Knowledge ETL] Processing new dataset {accession} via LLM...", flush=True)
        # æ¸…æ´—
        structured_data = self.clean_metadata_with_llm(f"Title: {raw_title}\nSummary: {raw_summary}")
        
        # å‘é‡åŒ– (å°†æ ‡é¢˜ã€ç–¾ç—…çŠ¶æ€å’Œæ¸…æ´—åçš„æ‘˜è¦æ‹¼æ¥ä½œä¸ºæ ¸å¿ƒæ£€ç´¢å†…å®¹)
        search_text = f"Title: {raw_title}. Disease: {structured_data.disease_state}. Summary: {structured_data.cleaned_summary}"
        embedding = self.get_embedding(search_text)
        
        # å†™å…¥æ•°æ®åº“
        dataset = PublicDataset(
            accession=accession,
            title=raw_title,
            summary=structured_data.cleaned_summary,
            organism=structured_data.organism,
            disease_state=structured_data.disease_state,
            sample_count=structured_data.sample_count,
            url=url,
            structured_metadata=structured_data.model_dump_json(exclude_none=True),
            embedding=embedding
        )
        db.add(dataset)
        db.commit()
        db.refresh(dataset)
        return dataset

    def semantic_search(self, db: Session, query: str, top_k: int = 5) -> List[PublicDataset]:
        """çº¯æœ¬åœ°çš„å‘é‡æ£€ç´¢ (å¤‡ç”¨)"""
        query_embedding = self.get_embedding(query)
        distance_threshold = 0.6 
        return db.exec(
            select(PublicDataset)
            .where(PublicDataset.embedding.cosine_distance(query_embedding) < distance_threshold)
            .order_by(PublicDataset.embedding.cosine_distance(query_embedding))
            .limit(top_k)
        ).all()

    # ğŸ‘‡ æ ¸å¿ƒå‡çº§ï¼šåŸºäº LLM å‚æ•°è®°å¿†çš„ç”Ÿæˆå¼æ£€ç´¢
    def agentic_geo_search(self, db: Session, user_query: str, top_k: int = 5) -> List[PublicDataset]:
        """
        è®©å¤§æ¨¡å‹ç›´æ¥åŒ–èº«æ£€ç´¢å¼•æ“ï¼Œä»è‡ªèº«å‚æ•°è®°å¿†ä¸­è¾“å‡ºåŒ¹é…çš„æ•°æ®é›†
        """
        print(f"ğŸ¤– [Agentic Search] Asking LLM to directly recall datasets for: '{user_query}'", flush=True)
        
        prompt = f"""You are an expert bioinformatics data curator.
The user is searching for public transcriptomic, genomic, or clinical datasets (like GEO, TCGA, ArrayExpress).
User query: "{user_query}"

Based on your vast training knowledge, provide up to {top_k} REAL and HIGHLY RELEVANT public datasets that match this query.
Ensure that the accession numbers (e.g., GSE12345) and summaries are as accurate as possible based on published literature.
"""
        try:
            # 1. ç›´æ¥è®©å¤§æ¨¡å‹æŒ‰æ ¼å¼åå‡ºæŸ¥åˆ°çš„æ•°æ®é›†åˆ—è¡¨
            search_result = self.instructor_client.chat.completions.create(
                model=self.llm_model,
                response_model=LLMDatasetSearchResult,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2 # è¾ƒä½çš„æ¸©åº¦ä»¥å‡å°‘å¤§æ¨¡å‹äº§ç”Ÿè™šå‡ GSE å·(å¹»è§‰)çš„æ¦‚ç‡
            )
            raw_datasets = search_result.datasets
        except Exception as e:
            print(f"âš ï¸ [Agentic Search] LLM direct retrieval failed: {e}", flush=True)
            return []

        # 2. å¯¹æ¯”æ•°æ®åº“å¹¶èšåˆè¿”å›ç»“æœ
        results = []
        for ds in raw_datasets:
            print(f"ğŸ” [Agentic Search] LLM suggested dataset: {ds.accession}", flush=True)
            # è°ƒç”¨å…¥åº“é€»è¾‘ï¼šå­˜åœ¨çš„ç›´æ¥å–æœ¬åœ°ï¼Œä¸å­˜åœ¨çš„ç»è¿‡æ¸…æ´—ä¸å‘é‡åŒ–åå…¥åº“
            dataset_record = self.ingest_geo_dataset(
                db=db,
                accession=ds.accession,
                raw_title=ds.title,
                raw_summary=ds.summary,
                url=ds.url or f"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={ds.accession}"
            )
            results.append(dataset_record)
        
        return results

    def import_to_project(self, db: Session, dataset_id: str, project_id: str, user_id: str):
        """ä¸€é”®å¯¼å…¥æ¨¡æ‹Ÿé€»è¾‘ï¼šå†™å…¥ç‰©ç†æ–‡ä»¶å¹¶å…³è”æ•°æ®åº“"""
        dataset = db.get(PublicDataset, dataset_id)
        if not dataset: raise ValueError("Dataset not found")
        
        folder_name = f"Imported_{dataset.accession}"
        folder_key = f"{project_id}/_folders/{uuid.uuid4()}/"
        new_folder = File(filename=folder_name, size=0, content_type="application/x-directory", is_directory=True, s3_key=folder_key, uploader_id=user_id)
        db.add(new_folder)
        db.commit()
        db.refresh(new_folder)
        db.add(ProjectFileLink(project_id=project_id, file_id=new_folder.id))
        db.commit()

        upload_root = os.getenv("UPLOAD_ROOT", "/data/uploads")
        save_dir = os.path.join(upload_root, str(project_id), folder_name)
        os.makedirs(save_dir, exist_ok=True)
        
        readme_name, readme_path = f"{dataset.accession}_README.md", os.path.join(save_dir, f"{dataset.accession}_README.md")
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(f"# {dataset.title}\n\n**Organism:** {dataset.organism}\n**Disease:** {dataset.disease_state}\n**Samples:** {dataset.sample_count}\n\n## Summary\n{dataset.summary}\n\n[View Original Source]({dataset.url})")
            
        meta_name, meta_path = f"{dataset.accession}_metadata.json", os.path.join(save_dir, f"{dataset.accession}_metadata.json")
        with open(meta_path, "w", encoding="utf-8") as f:
            f.write(dataset.structured_metadata)

        for fname, fpath in [(readme_name, readme_path), (meta_name, meta_path)]:
            db_file = File(filename=fname, size=os.path.getsize(fpath), content_type="text/plain" if fname.endswith(".md") else "application/json", s3_key=os.path.join(str(project_id), folder_name, fname), uploader_id=user_id, parent_id=new_folder.id)
            db.add(db_file)
            db.commit()
            db.refresh(db_file)
            db.add(ProjectFileLink(project_id=project_id, file_id=db_file.id))
            db.commit()
            
        return new_folder

knowledge_service = KnowledgeService()