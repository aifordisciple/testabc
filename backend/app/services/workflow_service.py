import os
import subprocess
import csv
import json
from uuid import UUID
from datetime import datetime
from sqlmodel import Session, select
from app.models.user import Analysis, Project, Sample, File, SampleFileLink, SampleSheet
# ğŸ‘‡ å¼•å…¥ WorkflowTemplate ä»¥è·å– AI ç”Ÿæˆçš„è„šæœ¬ä»£ç 
from app.models.bio import WorkflowTemplate

class WorkflowService:
    def __init__(self):
        # ä¼˜å…ˆè¯»å–å®¿ä¸»æœºçš„çœŸå®å·¥ä½œè·¯å¾„ (ç”¨äº Docker æŒ‚è½½æ˜ å°„)
        self.base_work_dir = os.getenv("HOST_WORK_DIR", "/app/workspace")
        
        if not os.path.exists(self.base_work_dir):
            try:
                os.makedirs(self.base_work_dir, exist_ok=True)
            except Exception as e:
                print(f"âš ï¸ Warning: Could not create work dir {self.base_work_dir}: {e}")

        # å®¿ä¸»æœºçœŸå®æ•°æ®æ ¹ç›®å½• (ç”¨äº Docker æŒ‚è½½æ˜ å°„åˆ° /data/uploads)
        self.host_data_root = os.getenv(
            "HOST_DATA_ROOT", 
            "/opt/data1/public/software/systools/autonome/autonome_data"
        )

    def generate_samplesheet(self, session: Session, sample_sheet_id: UUID, output_path: str):
        sheet = session.get(SampleSheet, sample_sheet_id)
        if not sheet:
            raise ValueError(f"Sample sheet {sample_sheet_id} not found")

        samples = session.exec(select(Sample).where(Sample.sample_sheet_id == sample_sheet_id)).all()
        if not samples:
            raise ValueError("No samples found in this sheet")

        rows = []
        for sample in samples:
            links = session.exec(select(SampleFileLink).where(SampleFileLink.sample_id == sample.id)).all()
            r1_path = ""
            r2_path = ""
            
            for link in links:
                file_rec = session.get(File, link.file_id)
                if not file_rec: continue
                if not file_rec.s3_key: continue
                    
                abs_path = os.path.join(self.host_data_root, file_rec.s3_key)
                
                if link.file_role == "R1":
                    r1_path = abs_path
                elif link.file_role == "R2":
                    r2_path = abs_path
            
            if r1_path:
                rows.append({
                    "sample_id": sample.name,
                    "r1_path": r1_path,
                    "r2_path": r2_path
                })

        if not rows:
             raise ValueError("No valid R1 files found for samples")

        with open(output_path, 'w', newline='', encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["sample_id", "r1_path", "r2_path"])
            writer.writeheader()
            writer.writerows(rows)

    def run_pipeline(self, session: Session, analysis_id: UUID):
        analysis = session.get(Analysis, analysis_id)
        if not analysis:
            raise ValueError(f"Analysis {analysis_id} not found")
            
        run_dir = os.path.join(self.base_work_dir, str(analysis.id))
        results_dir = os.path.join(run_dir, "results")
        
        os.makedirs(run_dir, exist_ok=True)
        os.makedirs(results_dir, exist_ok=True)
        
        log_file_path = os.path.join(run_dir, "analysis.log")

        def write_log(message: str):
            timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
            try:
                with open(log_file_path, "a", encoding="utf-8") as f:
                    f.write(f"[{timestamp}] {message}\n")
            except Exception:
                pass

        write_log(f"--- Task Started: {analysis.id} ---")
        write_log(f"ğŸ“‚ Work Dir: {run_dir}")

        analysis.status = "running"
        analysis.work_dir = run_dir
        session.add(analysis)
        session.commit()

        # è·å–æ•°æ®åº“ä¸­çš„æ¨¡æ¿ä»£ç å’Œç±»å‹
        template = session.exec(
            select(WorkflowTemplate).where(WorkflowTemplate.script_path == analysis.workflow)
        ).first()
        
        task_type = template.workflow_type if template else "PIPELINE"

        try:
            # ==========================================
            # ğŸ› ï¸ æ¨¡å¼ A: ç‹¬ç«‹è„šæœ¬å·¥å…· (TOOL) - Docker éš”ç¦»æ‰§è¡Œ
            # ==========================================
            if task_type == "TOOL":
                write_log("ğŸ› ï¸ Mode: Standalone Tool Task (Docker Isolated)")
                if not template or not template.source_code:
                    raise ValueError("Tool source code is missing in the database.")

                # 1. æå–å‚æ•°
                params_dict = json.loads(analysis.params_json) if analysis.params_json else {}
                write_log(f"âš™ï¸ Tool Parameters: {json.dumps(params_dict, ensure_ascii=False)}")

                code = template.source_code
                
                # 2. æ™ºèƒ½æ¢æµ‹è¯­è¨€ï¼Œé€‰æ‹©å¯¹åº”çš„ Docker æ•°æ®ç§‘å­¦é•œåƒ
                if "library(" in code or "<-" in code:
                    script_name = "script.R"
                    # rocker/tidyverse åŒ…å« R, ggplot2, dplyr ç­‰å¼ºå¤§ R åŒ…
                    docker_image = "autonome-tool-env:latest"
                    exec_cmd = ["Rscript", script_name]
                elif "use strict" in code or "perl " in code.lower():
                    script_name = "script.pl"
                    docker_image = "autonome-tool-env:latest"
                    exec_cmd = ["perl", script_name]
                else:
                    # é»˜è®¤ Python
                    script_name = "script.py"
                    # jupyter/datascience-notebook é¢„è£…äº† pandas, numpy, scipy, matplotlib, seaborn ç­‰
                    docker_image = "autonome-tool-env:latest"
                    exec_cmd = ["python", script_name]

                # ä¿å­˜è„šæœ¬åˆ°ç‰©ç†ç›®å½•
                script_path = os.path.join(run_dir, script_name)
                with open(script_path, "w", encoding="utf-8") as f:
                    f.write(code)
                write_log(f"ğŸ“„ Saved tool script as {script_name}")

                # 3. æ„é€ ä¼ é€’ç»™è„šæœ¬çš„å‚æ•° (--key val)
                script_args = []
                for key, val in params_dict.items():
                    if val is not None and str(val).strip() != "":
                        if isinstance(val, bool):
                            if val: script_args.append(f"--{key}")
                        else:
                            script_args.extend([f"--{key}", str(val)])

                # 4. æ„é€  Docker å®¹å™¨æ‰§è¡Œå‘½ä»¤ (Docker-in-Docker æŒ‚è½½)
                # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬åœ¨æ“ä½œå®¿ä¸»æœºçš„ Dockerï¼Œ-v å¿…é¡»ä½¿ç”¨å®¿ä¸»æœºçš„çœŸå®è·¯å¾„
                docker_run_cmd = [
                    "docker", "run", "--rm",
                    # ä½¿ç”¨ä¸å®¿ä¸»æœºç›¸åŒçš„è¿è¡Œè·¯å¾„
                    "-v", f"{self.base_work_dir}:{self.base_work_dir}",
                    # æŒ‚è½½çœŸå®çš„æºæ–‡ä»¶ç›®å½•åˆ°å®¹å™¨å†…çš„ /data/uploads ä¾›è¯»å–
                    "-v", f"{self.host_data_root}:/data/uploads",
                    # åˆ‡æ¢å·¥ä½œç›®å½•åˆ°å½“å‰ä»»åŠ¡æ–‡ä»¶å¤¹
                    "-w", run_dir,
                    # ä»¥ root èº«ä»½è¿è¡Œé˜²æ­¢æƒé™ä¸è¶³æ— æ³•å†™å…¥æ–‡ä»¶ (éƒ¨åˆ† Jupyter é•œåƒé»˜è®¤ç”¨æˆ·é root)
                    "-u", "root",
                    docker_image
                ] + exec_cmd + script_args

                write_log(f"ğŸ³ Launching Docker Container: {docker_image}")
                write_log(f"ğŸš€ Executing Command: {' '.join(docker_run_cmd)}")

                with open(log_file_path, "a", encoding="utf-8") as f:
                    result = subprocess.run(docker_run_cmd, cwd=run_dir, stdout=f, stderr=f, text=True)

                if result.returncode == 0:
                    analysis.status = "completed"
                    write_log("âœ… Tool Task Completed Successfully!")
                else:
                    analysis.status = "failed"
                    write_log(f"âŒ Tool Task Failed with exit code {result.returncode}")

            # ==========================================
            # ğŸ”— æ¨¡å¼ B: ç”Ÿä¿¡åˆ†ææµç¨‹ (PIPELINE) - Nextflow æ‰§è¡Œ
            # ==========================================
            else:
                write_log("ğŸ”— Mode: Nextflow Pipeline Task")
                if not analysis.sample_sheet_id:
                    raise ValueError("No SampleSheet associated for Pipeline task.")

                # 1. ç”Ÿæˆ SampleSheet
                write_log("ğŸ“ Generating samplesheet...")
                samplesheet_path = os.path.join(run_dir, "samplesheet.csv")
                self.generate_samplesheet(session, analysis.sample_sheet_id, samplesheet_path)
                write_log("âœ… Samplesheet generated.")

                # 2. ç”Ÿæˆ params.json
                params_path = os.path.join(run_dir, "params.json")
                params_dict = json.loads(analysis.params_json) if analysis.params_json else {}
                with open(params_path, "w", encoding="utf-8") as f:
                    json.dump(params_dict, f, indent=2)
                write_log(f"âš™ï¸ Pipeline Parameters loaded.")

                # 3. åŠ¨æ€åŠ è½½ Nextflow æºç 
                pipeline_path = os.path.abspath(f"pipelines/{analysis.workflow}/main.nf")
                
                if template and template.source_code:
                    write_log("ğŸ“„ Loading pipeline code from database.")
                    pipeline_path = os.path.join(run_dir, "main.nf")
                    with open(pipeline_path, "w", encoding="utf-8") as f:
                        f.write(template.source_code)
                    
                    if template.config_code:
                        with open(os.path.join(run_dir, "nextflow.config"), "w", encoding="utf-8") as f:
                            f.write(template.config_code)
                else:
                    if not os.path.exists(pipeline_path):
                        pipeline_path = os.path.abspath("pipelines/simple_demo/main.nf")
                        if not os.path.exists(pipeline_path):
                            raise ValueError("No pipeline script found.")

                # 4. æ„å»ºå¹¶æ‰§è¡Œ Nextflow å‘½ä»¤
                cmd = [
                    "nextflow",
                    "run", pipeline_path,
                    "--input", samplesheet_path,
                    "--outdir", results_dir,
                    "-params-file", params_path 
                ]
                
                write_log(f"ğŸš€ Executing Pipeline: {' '.join(cmd)}")
                
                with open(log_file_path, "a", encoding="utf-8") as f:
                    result = subprocess.run(cmd, cwd=run_dir, stdout=f, stderr=f, text=True)
                
                if result.returncode == 0:
                    analysis.status = "completed"
                    write_log("âœ… Pipeline Success!")
                else:
                    analysis.status = "failed"
                    write_log(f"âŒ Pipeline Failed with exit code {result.returncode}")
                
        except Exception as e:
            analysis.status = "error"
            write_log(f"ğŸ’¥ System Error: {str(e)}")
            
        finally:
            analysis.end_time = datetime.utcnow()
            session.add(analysis)
            session.commit()
            write_log("--- Task Finished ---")

workflow_service = WorkflowService()