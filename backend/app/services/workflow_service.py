import os
import subprocess
import csv
from uuid import UUID
from datetime import datetime
from sqlmodel import Session, select
from app.models.user import Analysis, Project, Sample, File, SampleFileLink, SampleSheet

class WorkflowService:
    def __init__(self):
        # === æ ¸å¿ƒä¿®æ”¹ï¼šæ”¯æŒ DooD (Docker-out-of-Docker) ===
        # ä¼˜å…ˆè¯»å–å®¿ä¸»æœºçš„çœŸå®å·¥ä½œè·¯å¾„ï¼Œä»¥ä¾¿ Nextflow å‘é€ç»™ Docker çš„è·¯å¾„åœ¨å®¿ä¸»æœºä¸Šæ˜¯æœ‰æ•ˆçš„
        # å¦‚æœæœªè®¾ç½®ï¼Œå›é€€åˆ°å®¹å™¨å†…éƒ¨è·¯å¾„ /app/workspace
        self.base_work_dir = os.getenv("HOST_WORK_DIR", "/app/workspace")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(self.base_work_dir):
            try:
                os.makedirs(self.base_work_dir, exist_ok=True)
            except Exception as e:
                print(f"âš ï¸ Warning: Could not create work dir {self.base_work_dir}: {e}")

        # å®¿ä¸»æœºæ•°æ®æ ¹ç›®å½• (ç”¨äº input files)
        self.host_data_root = os.getenv(
            "HOST_DATA_ROOT", 
            "/opt/data1/public/software/systools/autonome/autonome_data"
        )

    def generate_samplesheet(self, session: Session, sample_sheet_id: UUID, output_path: str):
        sheet = session.get(SampleSheet, sample_sheet_id)
        if not sheet:
            raise ValueError(f"Sample sheet {sample_sheet_id} not found")

        samples = session.exec(select(Sample).where(Sample.sample_sheet_id == sample_sheet_id)).all()
        if not samples:
            raise ValueError("No samples found in this sheet")

        rows = []
        for sample in samples:
            links = session.exec(select(SampleFileLink).where(SampleFileLink.sample_id == sample.id)).all()
            r1_path = ""
            r2_path = ""
            
            for link in links:
                file_rec = session.get(File, link.file_id)
                if not file_rec: continue
                if not file_rec.s3_key: continue
                    
                # æ‹¼æ¥ç»å¯¹è·¯å¾„
                abs_path = os.path.join(self.host_data_root, file_rec.s3_key)
                
                if link.file_role == "R1":
                    r1_path = abs_path
                elif link.file_role == "R2":
                    r2_path = abs_path
            
            if r1_path:
                rows.append({
                    "sample_id": sample.name,
                    "r1_path": r1_path,
                    "r2_path": r2_path
                })

        if not rows:
             raise ValueError("No valid R1 files found for samples")

        with open(output_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=["sample_id", "r1_path", "r2_path"])
            writer.writeheader()
            writer.writerows(rows)

    def run_pipeline(self, session: Session, analysis_id: UUID):
        analysis = session.get(Analysis, analysis_id)
        if not analysis:
            raise ValueError(f"Analysis {analysis_id} not found")
            
        # ä½¿ç”¨ base_work_dir (å¯èƒ½æ˜¯å®¿ä¸»æœºè·¯å¾„)
        run_dir = os.path.join(self.base_work_dir, str(analysis.id))
        results_dir = os.path.join(run_dir, "results")
        
        os.makedirs(run_dir, exist_ok=True)
        os.makedirs(results_dir, exist_ok=True)
        
        log_file_path = os.path.join(run_dir, "analysis.log")

        def write_log(message: str):
            timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
            try:
                with open(log_file_path, "a") as f:
                    f.write(f"[{timestamp}] {message}\n")
            except Exception:
                pass

        write_log(f"--- Analysis Started: {analysis.id} ---")
        write_log(f"ğŸ“‚ Work Dir: {run_dir}")

        if not analysis.sample_sheet_id:
            write_log("âŒ Error: No SampleSheet associated.")
            analysis.status = "failed"
            session.add(analysis)
            session.commit()
            return

        try:
            write_log("ğŸ“ Generating samplesheet...")
            samplesheet_path = os.path.join(run_dir, "samplesheet.csv")
            self.generate_samplesheet(session, analysis.sample_sheet_id, samplesheet_path)
            write_log("âœ… Samplesheet generated.")
        except Exception as e:
            write_log(f"âŒ Error generating samplesheet: {e}")
            analysis.status = "failed"
            session.add(analysis)
            session.commit()
            return

        # è¿™é‡Œçš„ pipeline path ä»ç„¶æ˜¯å®¹å™¨å†…çš„ç›¸å¯¹è·¯å¾„ /app/pipelines/...
        # åªè¦ backend å®¹å™¨èƒ½è¯»åˆ°å³å¯ï¼ŒNextflow ä¼šå¤„ç†
        pipeline_name = analysis.workflow 
        pipeline_path = os.path.abspath(f"pipelines/{pipeline_name}/main.nf")
        
        if not os.path.exists(pipeline_path):
             pipeline_path = os.path.abspath("pipelines/simple_demo/main.nf")
             if not os.path.exists(pipeline_path):
                 write_log("âŒ No pipeline script found.")
                 analysis.status = "failed"
                 session.add(analysis)
                 session.commit()
                 return

        cmd = [
            "nextflow", "run", pipeline_path,
            "--input", samplesheet_path,
            "--outdir", results_dir
        ]
        
        analysis.status = "running"
        analysis.work_dir = run_dir
        session.add(analysis)
        session.commit()
        
        write_log(f"ğŸš€ Executing: {' '.join(cmd)}")
        
        try:
            with open(log_file_path, "a") as f:
                result = subprocess.run(
                    cmd, 
                    cwd=run_dir, 
                    stdout=f, 
                    stderr=f, 
                    text=True
                )
            
            if result.returncode == 0:
                analysis.status = "completed"
                write_log("âœ… Pipeline Success!")
            else:
                analysis.status = "failed"
                write_log(f"âŒ Pipeline Failed with exit code {result.returncode}")
                
        except Exception as e:
            analysis.status = "error"
            write_log(f"ğŸ’¥ System Error: {str(e)}")
            
        finally:
            analysis.end_time = datetime.utcnow()
            session.add(analysis)
            session.commit()
            write_log("--- Analysis Finished ---")

workflow_service = WorkflowService()