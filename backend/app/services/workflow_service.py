import os
import subprocess
import csv
from uuid import UUID
from datetime import datetime
from sqlmodel import Session, select
from app.models.user import Analysis, Project, Sample, File, SampleFileLink, SampleSheet

class WorkflowService:
    def __init__(self, base_work_dir: str = "workspace"):
        # workspace ç›®å½•ç”¨äºå­˜æ”¾ nextflow çš„è¿è¡Œæ—¥å¿—å’Œè¿‡ç¨‹æ–‡ä»¶
        self.base_work_dir = os.path.abspath(base_work_dir)
        os.makedirs(self.base_work_dir, exist_ok=True)

        # âš ï¸ æ ¸å¿ƒé…ç½®ï¼šå®¿ä¸»æœºä¸Šçš„æ•°æ®æ ¹ç›®å½• (Host Path)
        # è¿™æ˜¯ Nextflow è¿›ç¨‹ï¼ˆè¿è¡Œåœ¨å®¿ä¸»æœºä¸Šï¼‰å»å¯»æ‰¾æ•°æ®æ–‡ä»¶çš„åœ°æ–¹
        # è¯·åŠ¡å¿…æ ¸å¯¹è¿™ä¸ªè·¯å¾„æ˜¯å¦æ­£ç¡®ï¼
        self.host_data_root = os.getenv(
            "HOST_DATA_ROOT", 
            "/opt/data1/public/software/systools/autonome/autonome_data"
        )

    def generate_samplesheet(self, session: Session, project_id: UUID, output_path: str):
        """
        ç”Ÿæˆ samplesheet.csv (æœ¬åœ°ç›´å­˜ç‰ˆ - é›¶æ‹·è´)
        Nextflow å°†ç›´æ¥è¯»å–å®¿ä¸»æœºç¡¬ç›˜ä¸Šçš„åŸå§‹æ–‡ä»¶
        """
        # 1. æŸ¥æ‰¾ SampleSheet
        sheets = session.exec(select(SampleSheet).where(SampleSheet.project_id == project_id)).all()
        sheet_ids = [s.id for s in sheets]
        
        if not sheet_ids:
            raise ValueError("No sample sheets found")

        # 2. æŸ¥æ‰¾ Samples
        samples = session.exec(select(Sample).where(Sample.sample_sheet_id.in_(sheet_ids))).all()
        
        if not samples:
            raise ValueError("No samples found")

        rows = []
        for sample in samples:
            links = session.exec(select(SampleFileLink).where(SampleFileLink.sample_id == sample.id)).all()
            
            r1_path = ""
            r2_path = ""
            
            for link in links:
                file_rec = session.get(File, link.file_id)
                if not file_rec: continue
                
                # === æ ¸å¿ƒä¿®æ”¹ï¼šè·¯å¾„æ‹¼æ¥ ===
                # æ•°æ®åº“é‡Œçš„ s3_key ç°åœ¨å­˜å‚¨çš„æ˜¯ "project_id/filename"
                if not file_rec.s3_key: 
                    print(f"âš ï¸ File {file_rec.filename} missing path info")
                    continue
                    
                # æ‹¼æ¥å‡ºå®¿ä¸»æœºä¸Šçš„ç»å¯¹è·¯å¾„
                # ä¾‹å¦‚: /opt/.../autonome_data/UUID/reads_1.fq.gz
                abs_path = os.path.join(self.host_data_root, file_rec.s3_key)
                
                # å¯é€‰ï¼šæ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
                # æ³¨æ„ï¼šPython ä»£ç æ˜¯åœ¨ Docker é‡Œè·‘çš„ï¼Œå®ƒèƒ½çœ‹åˆ°çš„è·¯å¾„æ˜¯ /data/uploads/...
                # ä½†æˆ‘ä»¬è¦ç”Ÿæˆç»™ Host ä¸Š Nextflow ç”¨çš„è·¯å¾„ï¼Œæ‰€ä»¥è¿™é‡Œä¸èƒ½ç”¨ os.path.exists(abs_path) æ¥åˆ¤æ–­
                # é™¤éæˆ‘ä»¬åšä¸€ä¸ª Docker è·¯å¾„åˆ° Host è·¯å¾„çš„æ˜ å°„æ£€æŸ¥ï¼Œè¿™é‡Œæš‚ä¸”ç›¸ä¿¡ DB è®°å½•
                
                if link.file_role == "R1":
                    r1_path = abs_path
                elif link.file_role == "R2":
                    r2_path = abs_path
            
            if r1_path:
                rows.append({
                    "sample_id": sample.name,
                    "r1_path": r1_path,
                    "r2_path": r2_path
                })

        # 3. å†™å…¥ CSV
        with open(output_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=["sample_id", "r1_path", "r2_path"])
            writer.writeheader()
            writer.writerows(rows)

    def run_pipeline(self, session: Session, analysis_id: UUID):
        """é©±åŠ¨ Nextflow"""
        analysis = session.get(Analysis, analysis_id)
        if not analysis:
            raise ValueError(f"Analysis {analysis_id} not found")
            
        # å‡†å¤‡ç›®å½•
        run_dir = os.path.join(self.base_work_dir, str(analysis.id))
        results_dir = os.path.join(run_dir, "results")
        
        os.makedirs(run_dir, exist_ok=True)
        os.makedirs(results_dir, exist_ok=True)
        
        log_file_path = os.path.join(run_dir, "analysis.log")

        def write_log(message: str):
            timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
            with open(log_file_path, "a") as f:
                f.write(f"[{timestamp}] {message}\n")

        write_log(f"--- Analysis Started: {analysis.id} ---")

        # 1. ç”Ÿæˆ SampleSheet
        try:
            write_log("ğŸ“ Generating samplesheet (Direct Path Mode)...")
            samplesheet_path = os.path.join(run_dir, "samplesheet.csv")
            
            self.generate_samplesheet(session, analysis.project_id, samplesheet_path)
            
            write_log("âœ… Samplesheet generated.")
        except Exception as e:
            write_log(f"âŒ Error generating samplesheet: {e}")
            analysis.status = "failed"
            session.add(analysis)
            session.commit()
            return

        # 2. å®šä½æµç¨‹
        pipeline_name = analysis.workflow 
        pipeline_path = os.path.abspath(f"pipelines/{pipeline_name}/main.nf")
        
        if not os.path.exists(pipeline_path):
             write_log(f"âš ï¸ Workflow {pipeline_name} not found, falling back to simple_demo")
             pipeline_path = os.path.abspath("pipelines/simple_demo/main.nf")

        # 3. æ„å»ºå‘½ä»¤
        cmd = [
            "nextflow", "run", pipeline_path,
            "--input", samplesheet_path,
            "--outdir", results_dir,
            "-with-docker",
        ]
        
        analysis.status = "running"
        analysis.work_dir = run_dir
        session.add(analysis)
        session.commit()
        
        write_log(f"ğŸš€ Executing: {' '.join(cmd)}")
        
        try:
            with open(log_file_path, "a") as f:
                result = subprocess.run(
                    cmd, 
                    cwd=run_dir, 
                    stdout=f, 
                    stderr=f, 
                    text=True
                )
            
            if result.returncode == 0:
                analysis.status = "completed"
                write_log("âœ… Pipeline Success!")
            else:
                analysis.status = "failed"
                write_log(f"âŒ Pipeline Failed with exit code {result.returncode}")
                
        except Exception as e:
            analysis.status = "error"
            write_log(f"ğŸ’¥ System Error: {str(e)}")
            
        finally:
            analysis.end_time = datetime.utcnow()
            session.add(analysis)
            session.commit()
            write_log("--- Analysis Finished ---")

workflow_service = WorkflowService()